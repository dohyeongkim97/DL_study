{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d08cd3",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a66b87",
   "metadata": {},
   "source": [
    "Convolution Neural Network. A sort of ANN which is mainly used for computer vision field like Image Recognition etc.\n",
    "\n",
    "Specialised for extracting local features of input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd7770",
   "metadata": {},
   "source": [
    "Also can be used for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866487b7",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09471be",
   "metadata": {},
   "source": [
    "aka Kernel or Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133fa0a9",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77778a",
   "metadata": {},
   "source": [
    "adding fixed values to the edges of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e4524",
   "metadata": {},
   "source": [
    "## Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d036dc",
   "metadata": {},
   "source": [
    "the size which the filter moves.\n",
    "\n",
    "if stride be bigger, the output size be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6ebfb",
   "metadata": {},
   "source": [
    "## Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021cad0c",
   "metadata": {},
   "source": [
    "when the input and filter composed by 3rd demension, channel make the values be calculated on same level. \n",
    "\n",
    "can expand the extracted feature while maintaning loc info of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83fe85",
   "metadata": {},
   "source": [
    "usually set by conv-level, and it's different by the structure or object of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f24808",
   "metadata": {},
   "source": [
    "in case that there are many output channels, each of channel can get each other features by input data. therefore model can get more of treats of features. so that it can solve some difficult problemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb86ce",
   "metadata": {},
   "source": [
    "## Dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a3406",
   "metadata": {},
   "source": [
    "put terms into the filter and input data. usually 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a7595",
   "metadata": {},
   "source": [
    "CNN layer classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d571a",
   "metadata": {},
   "source": [
    "```\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size, ( filter size )\n",
    "    stride=1, ( move )\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1, ( when group == 1, in channels and out channels bounded in one group. if more, group devided by several smaller groups, and conv)\n",
    "    bias=True,\n",
    "    padding_mode='zeros'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbef55",
   "metadata": {},
   "source": [
    "$L_{\\text{out}} = \\left[ \\frac{L_{\\text{in}} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel} - 1) - 1}{\\text{stride}} + 1 \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c431a",
   "metadata": {},
   "source": [
    "## Activation Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25146fa4",
   "metadata": {},
   "source": [
    "a specific value gained adopting activate fucntion to specific feature map in CNN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6803b",
   "metadata": {},
   "source": [
    "by several layer of activation map, NN can get abstract features of input images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95a130",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce9fcc",
   "metadata": {},
   "source": [
    "feature map size reducing. compressing input data info and reduce calc power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea26b6",
   "metadata": {},
   "source": [
    "max pooling vs avg pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d42b79",
   "metadata": {},
   "source": [
    "``` \n",
    "pool = torch.nn.MaxPool2d(\n",
    "    kernel_size,\n",
    "    stride=None,\n",
    "    padding=0,\n",
    "    dilation=1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca3f68",
   "metadata": {},
   "source": [
    "```\n",
    "pool = ttorch.nn.AvgPool2d(\n",
    "    kernel_size,\n",
    "    stride=None,\n",
    "    padding=0,\n",
    "    count_include_pad=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839737e",
   "metadata": {},
   "source": [
    "## Fully Connected Layer, FC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a503f9",
   "metadata": {},
   "source": [
    "a status that every input node each connnected to every output node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663d78b",
   "metadata": {},
   "source": [
    "e.g. 2 dim data => 1 dim vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf86940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(32*32*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1\n",
    "        x = self.conv2\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116815e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16219e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "        )\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "        \n",
    "        conv=[]\n",
    "        \n",
    "        for size in filter_sizes:\n",
    "            conv.appendd(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels = embedding_dim,\n",
    "                        out_channels=1,\n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "        \n",
    "        output_size = len(filter_sizes)\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        \n",
    "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "        \n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
    "classifier = sentence_classifier(\n",
    "    pretrained_embedding = init_embeddings,\n",
    "    filter_sizes = filter_sizes,\n",
    "    max_length = max_length\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = opttim.Adam(classifier.parameters(0, lr=0.001))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
