{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba45522",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c157865",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd8cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0667f46",
   "metadata": {},
   "source": [
    "```\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings,\n",
    "    embedding_dim,\n",
    "    padding_idx = None,\n",
    "    max_norm = None,\n",
    "    norm_type = 2.0\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c825b",
   "metadata": {},
   "source": [
    "num_embeddings = 단어 사전의 크기\n",
    "\n",
    "embedding_dim = 임베딩 벡터의 차원 수. 임베딩 벡터의 크기\n",
    "\n",
    "padding_idx = 패딩 토큰의 인덱스를지정하여 해당 인덱스의 임베딩 벡터를 0으로 설정. 입력한 문장들을 일정 길이로 맞추는 역할을 수행.\n",
    "\n",
    "norm_type = 임베딩 벡터의 크기를 제한하는 방법을 선택. 기본값은 2(Ridge, L2). 1을 선택하면 L1(Lasso) 방식을 사용.\n",
    "\n",
    "max_norm = embedding vector의 최대 크기를 지정. 각 임베딩 벡터의 크기가 최대 norm값 이상이라면 임베딩 벡터를 최대 norm 크기로 잘라내고 크기를 감소."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96abcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c40b9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaSkipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        output = self.linear(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c878e6",
   "metadata": {},
   "source": [
    "계층적 softmax나 negative sampling과 같은 효율적인 기법이 사용되지 않은 기본 형식의 skip-gram 모델.\n",
    "\n",
    "입력 단어와 주변 단어를 lookup table에서 가져와서 내적을 계산하고 손실 함수를 통해 예측 오차를 최소화하는 방식으로 학습."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12f190",
   "metadata": {},
   "source": [
    "기본 형식의 skip-gram 모델을 선언했다면, 모델 학습에 사용할 데이터세트를 불러와야 함.\n",
    "\n",
    "이 경우 데이터세트는 kopora library의 네이버 영화 리뷰 감성 분석 데이터셋."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e26dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152032a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "corpus = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc085564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>마무리는 또 왜이래</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                                    굳 ㅋ      1\n",
       "1                                   GDNTOPCLASSINTHECLUB      0\n",
       "2                 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3                       지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4      3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n",
       "...                                                  ...    ...\n",
       "49995          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함      1\n",
       "49996       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO      0\n",
       "49997                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n",
       "49998     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네      0\n",
       "49999                                         마무리는 또 왜이래      0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817dea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
    "\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56915bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9236183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0c65e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(corpus = tokens, n_vocab=5000, special_tokens = ['<unk>'])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82585f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pairs(tokens, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            window_start = max(0, idx-window_size)\n",
    "            window_end = min(sentence_length, idx+window_size+1)\n",
    "            center_word = sentence[idx]\n",
    "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
    "            \n",
    "            for context_word in context_words:\n",
    "                pairs.append([center_word, context_word])\n",
    "                \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b03a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = get_word_pairs(tokens, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ccafbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da00cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐'], ['야', '이'], ['야', '평점'], ['이', '뭐'], ['이', '야'], ['이', '평점']]\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ca757f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2589946"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa1d9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_pairs(word_pairs, token_to_id):\n",
    "    pairs = []\n",
    "    unk_index = token_to_id['<unk>']\n",
    "    for word_pair in word_pairs:\n",
    "        center_word, context_word = word_pair\n",
    "        center_index = token_to_id.get(center_word, unk_index)\n",
    "        context_index = token_to_id.get(context_word, unk_index)        \n",
    "        pairs.append([center_index, context_index])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e5f04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pairs = get_index_pairs(word_pairs, token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ede7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n"
     ]
    }
   ],
   "source": [
    "print(index_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bab15bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fac3d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Temp\\ipykernel_8480\\652414755.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  index_pairs = torch.tensor(index_pairs)\n"
     ]
    }
   ],
   "source": [
    "index_pairs = torch.tensor(index_pairs)\n",
    "center_indexes = index_pairs[:, 0]\n",
    "content_indexes = index_pairs[:, 1]\n",
    "\n",
    "dataset = TensorDataset(center_indexes, content_indexes)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5bf6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "70130181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "word2vec = VanillaSkipgram(vocab_size = len(token_to_id), embedding_dim = 128).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82eb555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1, Cost: 5.792\n",
      "Epoch :    2, Cost: 5.783\n",
      "Epoch :    3, Cost: 5.775\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    cost = 0.0\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        logits = word2vec(input_ids)\n",
    "        loss = criterion(logits, target_ids)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cost += loss\n",
    "        \n",
    "    cost = cost/len(dataloader)\n",
    "    \n",
    "    if epoch > 1:\n",
    "        if previous - 0.0001 > cost:\n",
    "            print(f\"Epoch : {epoch+1:4d}, Cost: {cost:.3f}\")\n",
    "            break\n",
    "    previous = cost\n",
    "    \n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost: {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c4ce3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a4de7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, embedding in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d39781d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연기\n",
      "[-1.1470925  -0.45353258 -1.4100281  -0.4359115   0.45776555 -1.0378495\n",
      " -0.5793153  -1.0564619  -0.0040138   0.31449902  0.8191109   0.6205507\n",
      "  0.25863805  0.90914685 -1.034102    1.1804768  -0.8231319  -0.28133342\n",
      " -0.73907053 -0.3992155   0.10631946 -0.12566547  0.98546356 -0.24158162\n",
      " -0.16468984 -0.7064888   0.00693423 -1.4487778   1.2125809  -0.16144711\n",
      "  0.6580337   0.29140827  0.5636844   0.23774803 -0.5729031  -1.0407064\n",
      " -1.1420559  -0.48986226 -0.5752811  -0.01549503 -0.39826065  0.521287\n",
      " -0.22195017 -0.00792905  0.3537114   2.490282   -0.36432886  0.89823234\n",
      "  0.13147224 -0.10786215 -1.5473667  -0.94905186  1.115509   -0.04999183\n",
      " -1.806789   -0.34175512  0.8161983  -0.5417446  -0.20808673 -1.8825325\n",
      " -1.1143032   0.10481103  1.135397    0.16712715 -0.82443273 -0.3895439\n",
      " -0.53490967 -1.7814435   0.16670318  0.15598522  0.13907772 -0.6384126\n",
      " -1.8532416   0.82332444  0.4496294   0.11585472  1.7476325   0.6590356\n",
      "  0.85262185  0.1694762   0.14345947 -0.78168476  0.64653945  0.13914694\n",
      " -1.122123    0.42578438  0.5572788   0.4499702  -0.46305794  0.23856813\n",
      "  0.7939863   0.12665154 -1.4955126   0.04767629  0.22270529 -0.79019696\n",
      " -0.1366494   0.7246961  -0.87352186  0.4351406  -1.4683204   2.125844\n",
      "  1.6973016  -0.9112073   0.20072468  0.8397447   0.89698446  0.77767116\n",
      " -1.125021    0.1451709  -1.4818686  -1.6853642   1.9182452   1.6914431\n",
      "  0.02239738  0.60927325  0.34109917 -1.016018   -0.44655716 -0.18612784\n",
      "  0.4856196  -0.14289856 -0.17133524 -0.06180555  1.0963583  -0.13239767\n",
      "  0.78933454  0.6002351 ]\n"
     ]
    }
   ],
   "source": [
    "index = 30\n",
    "token = vocab[30]\n",
    "token_embedding = token_to_embedding[token]\n",
    "print(token)\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd716d",
   "metadata": {},
   "source": [
    "word2vec 모델의 임베딩 행렬을 이용, 각 단어의 embedding 값을 매핑."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42736f",
   "metadata": {},
   "source": [
    "임베딩의 유사도 측정에 있어서는 cosine similarity를 일반적으로 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41d080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec2d261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
    "    \n",
    "    return cosine\n",
    "\n",
    "def top_n_index(cosine_matrix, n):\n",
    "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
    "    top_n = closest_indexes[1 : n+1]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "98dc16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
    "top_n = top_n_index(cosine_matrix, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c98b5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 words\n",
      "있는지 - sim: 0.3006\n",
      "재밌던 - sim: 0.2864\n",
      "연기력 - sim: 0.2824\n",
      "살리지 - sim: 0.2792\n",
      "cg - sim: 0.2653\n"
     ]
    }
   ],
   "source": [
    "print('5 words')\n",
    "for index in top_n:\n",
    "    print(f\"{id_to_token[index]} - sim: {cosine_matrix[index]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473d165",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b8cae",
   "metadata": {},
   "source": [
    "using hierarchical softmax or negative sampling to be more effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "35b7ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f6fbb",
   "metadata": {},
   "source": [
    "adopting negative sampling by Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf6788",
   "metadata": {},
   "source": [
    "```\n",
    "word2vec = gensim.models.Word2Vec(\n",
    "    sentences = None, # 학습 데이터\n",
    "    corpus_file = None, # 파일 경로. 입력 문장(sentence) 대신 사용\n",
    "    vector_size = 100, # 임베딩 벡터 크기\n",
    "    alpha = 0.025 # Word2Vec 학습률\n",
    "    window = 5, # 학습 데이터 생성 윈도우 크기. e.g. window=3이라면, 중심 단어로부터 3거리만큼의 단어까지 고려\n",
    "    min_count = 5, # 학습에 사용할 단어의 최소 빈도. 말뭉치 내 최소 빈도만큼 등장하지 아니한 경우라면 학습에 사용하지 아니함.\n",
    "    workers = 3, # 빠른 학습을 위해 병렬 학습할 스레드의 수\n",
    "    sg = 0, # Skip-gram 모델의 사용 여부를 설정. 1이면 skip gram, 0이면 CBoW 모델\n",
    "    hs = 0, # 계층적 소프트맥스. \n",
    "    cbow_mean = 1, # CBoW 모델로 구성 시 사용되는 하이퍼파라미터. 주변 단어를 합쳐 하나의 벡터로 만들 때, 합한 벡터의 평균화 여부를 설정. 1이면 평균화\n",
    "    negative = 5, #  \n",
    "    ns_exponent = 0.75, # 네거티브 샘플링 확률의 지수. \n",
    "    max_final_vocab = None, # 단어 사전의 최대 크기. 최소 빈도를 충족하는 단어가 최대 최종 단어 사전보다 많다면, 등장률이 높은 순으로 사전을 구축\n",
    "    epochs = 5,\n",
    "    batch_words = 10000 # 몇 개의 단어로 학습 배치를 구성할지 결정. 컴퓨팅 자원이 모자란다면, 배치 단어 수를 낮추어 학습을 진행.\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68d89240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "faf63cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(\n",
    "    sentences = tokens,\n",
    "    vector_size=128,  \n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    epochs=3,\n",
    "    max_final_vocab=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ee7769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.save('./word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2626a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6bcb7d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25246373 -0.3491199   0.38122186  0.25815445 -0.05388842 -0.04009622\n",
      " -0.05378585 -0.22577009 -0.6477572   0.42385954 -0.05005813 -0.25666878\n",
      " -0.16922015 -0.07851492  0.05295365 -0.02408049 -0.21038438  0.26429063\n",
      " -0.14678991  0.12876222  0.48424977  0.06698535 -0.32169378 -0.06314953\n",
      " -0.23801267  0.08292168 -0.39624816  0.05241839  0.28025323 -0.08086042\n",
      " -0.33615294  0.1661649   0.5135952  -0.05084412 -0.16073489  0.21309592\n",
      "  0.23913004 -0.11016405  0.05802022 -0.14048728  0.00995715  0.2961721\n",
      "  0.03572152 -0.38330662 -0.5463036   0.0364121  -0.62656003  0.07114148\n",
      "  0.08507526 -0.00488687  0.589722    0.07030232  0.25247985  0.28395632\n",
      " -0.34245312 -0.1888582  -0.01957356  0.08395969 -0.04283346  0.14319025\n",
      " -0.04381518 -0.07038638  0.060624    0.28508776 -0.4624694   0.13415839\n",
      "  0.00784608  0.2480188   0.47224653 -0.20374437 -0.23252682 -0.25931555\n",
      " -0.39035228  0.17183472 -0.11554734 -0.208209   -0.22948785 -0.39710212\n",
      " -0.10575233  0.15993853 -0.3142133   0.06665111  0.3417644   0.48174673\n",
      "  0.22964312  0.00288855  0.41628557 -0.52501357 -0.07434731  0.04197197\n",
      " -0.16598739  0.14213073 -0.03498432  0.13560586  0.23810776  0.0178275\n",
      " -0.17568013 -0.2995944  -0.17846192 -0.779346   -0.25684854 -0.09770235\n",
      "  0.19885711 -0.515749    0.0814383   0.40049762  0.5034687   0.20895116\n",
      "  0.11139756 -0.30574512  0.36141652 -0.3279599  -0.4656567   0.21400861\n",
      "  0.169015   -0.45169237  0.5807277   0.279027   -0.01185692  0.32835364\n",
      " -0.13617183 -0.20847207  0.30738524 -0.05158995  0.01427162  0.07755034\n",
      " -0.44582072 -0.19542049]\n",
      "[('연기력', 0.8078137636184692), ('캐스팅', 0.7534756064414978), ('몸매', 0.720723032951355), ('연기자', 0.7174034118652344), ('조연', 0.7171277403831482), ('목소리', 0.7054124474525452), ('몰입도', 0.7014158368110657), ('어색', 0.6961895227432251), ('발연기', 0.6944844126701355), ('연', 0.6933286190032959)]\n",
      "0.80781376\n"
     ]
    }
   ],
   "source": [
    "word = '연기'\n",
    "print(word2vec.wv[word])\n",
    "print(word2vec.wv.most_similar(word, topn=10))\n",
    "print(word2vec.wv.similarity(w1=word, w2='연기력'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1749f",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24299c8",
   "metadata": {},
   "source": [
    "an opensource embedding model. algorithme for text classification and text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70ffed",
   "metadata": {},
   "source": [
    "this technique is similar with word2vec, but has higher accuracy and efficiency, cause it calculate sub-word of target word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a00264",
   "metadata": {},
   "source": [
    "use '<', '>', to vectorise words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292265e7",
   "metadata": {},
   "source": [
    "symbol attached word is being distributed to subword set by using N-gram. e.g. '서울특별시' => '서울', '울특', '특별', '별시' etc. and also subword set contains the origin word('서울특별시')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e478b",
   "metadata": {},
   "source": [
    "```\n",
    "fasttext = gensim.Models.FastText(\n",
    "    sentences=None,\n",
    "    corpus_file=None,\n",
    "    vector_size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=3,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    cbow_mean=1,\n",
    "    negative=5,\n",
    "    ns_exponent=0.75,\n",
    "    max_final_vocab=None,\n",
    "    epochs=5,\n",
    "    batch_words=10000,\n",
    "    min_n=3,\n",
    "    max_n=6\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f73083",
   "metadata": {},
   "source": [
    "ngram에서 최소n이 2고, 최대n이 4라면, 2-gram, 3-gram, 4-gram으로 나뉘어 하위단어집합을 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0441a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
