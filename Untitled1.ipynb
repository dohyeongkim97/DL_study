{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd882df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Korpora\n",
    "from Korpora import Korpora\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d464030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea93370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : KakaoBrain\n",
      "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
      "    References :\n",
      "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
      "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
      "           (https://arxiv.org/abs/2004.03289)\n",
      "\n",
      "    This is the dataset repository for our paper\n",
      "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
      "    (https://arxiv.org/abs/2004.03289)\n",
      "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
      "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\multinli.train.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\snli_1.0_train.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\xnli.dev.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\xnli.test.ko.tsv\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('kornli')\n",
    "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba962b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [sentence.split() for sentence in corpus_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0937d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(\n",
    "    sentences = tokens,\n",
    "    vector_size = 128,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    sg = 1,\n",
    "    epochs = 3,\n",
    "    min_n = 2,\n",
    "    max_n = 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fd2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = '사랑해요'\n",
    "oov_vector = fasttext.wv[oov_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279e88b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[('사랑해', 0.9089314341545105), ('사랑', 0.8616943359375), ('사랑한', 0.8494669198989868), ('사랑해서', 0.847015380859375), ('사랑해.', 0.8459593653678894)]\n"
     ]
    }
   ],
   "source": [
    "print(oov_token in fasttext.wv.index_to_key)\n",
    "print(fasttext.wv.most_similar(oov_vector, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ddeea",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b75a1e",
   "metadata": {},
   "source": [
    "```\n",
    "rnn = torch.nn.RNN(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers = 1,\n",
    "    nomlinearity='tanh',\n",
    "    bias=False,\n",
    "    batch_first = True,\n",
    "    dropout = 0,\n",
    "    bidirectional = False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71198de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c153a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128\n",
    "output_size = 256\n",
    "num_layers = 3\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf03d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.RNN(\n",
    "    input_size = input_size,\n",
    "    hidden_size = output_size,\n",
    "    num_layers = num_layers,\n",
    "    nonlinearity='tanh',\n",
    "    batch_first = True,\n",
    "    bidirectional=bidirectional,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31cd8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "sequence_len=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311ed9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(batch_size, sequence_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9580e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.rand(num_layers * (int(bidirectional)+1), batch_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93211fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hidden = model(inputs, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2479e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 512])\n",
      "torch.Size([6, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042a6f9",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f5ae6",
   "metadata": {},
   "source": [
    "Long Short Term Memory: RNN 모델이 갖던 기억력 부족과 Gradient Vanishing 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75119d49",
   "metadata": {},
   "source": [
    "RNN 모델은 장기 의존성 문제(Long Term Dependencies) 문제가 발생 가능. 활성화함수로 사용되는 tanh 함수나 ReLU 함수 특성으로 인해 역전파 과정에서 기울기 소실이나 폭주도 발생 가능함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a4e79",
   "metadata": {},
   "source": [
    "LSTM 모델은 순환 싱경망과 비슷한 구조를 가지나, Memory cell과 Gate 구조의 도입으로 상기한 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fd139",
   "metadata": {},
   "source": [
    "```\n",
    "lstm = torch.nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers=1,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    dropout=0,\n",
    "    bidirectional=False,\n",
    "    proj_size=0\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd8b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d63103a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:879: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at ..\\aten\\src\\ATen\\native\\RNN.cpp:1493.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    }
   ],
   "source": [
    "input_size=128\n",
    "output_size=256\n",
    "num_layers = 3\n",
    "bidirectional=True\n",
    "proj_size=64\n",
    "\n",
    "model = nn.LSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional=bidirectional,\n",
    "    proj_size=proj_size\n",
    ")\n",
    "\n",
    "batch_size=4\n",
    "sequence_len=6\n",
    "\n",
    "inputs=torch.randn(batch_size, sequence_len, input_size)\n",
    "h0=torch.rand(\n",
    "    num_layers * (int(bidirectional)+1),\n",
    "    batch_size,\n",
    "    proj_size if proj_size > 0 else output_size,\n",
    ")\n",
    "c0 = torch.rand(num_layers * (int(bidirectional)+1), batch_size, output_size)\n",
    "\n",
    "outputs, (hn, cn) = model(inputs, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c775b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 128])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf149422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4, 64])\n",
      "torch.Size([6, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(hn.shape)\n",
    "print(cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f03911",
   "metadata": {},
   "source": [
    "# P/N classification model by using RNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afffa8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type='lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx = 0\n",
    "        )\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f611cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36eae260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5876833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3786b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "train_size: 45000\n",
      "test_size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(train.head(5).to_markdown())\n",
    "print(f'train_size: {len(train)}')\n",
    "print(f'test_size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3db1ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c17a57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "        vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=['<pad>', '<unk>'])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcff9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38ab6f",
   "metadata": {},
   "source": [
    "# int encoding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75a3683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c267184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8092e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = token_to_id['<unk>']\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6e8c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e840e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b95c913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Temp\\ipykernel_8156\\381660097.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\dohyeong\\AppData\\Local\\Temp\\ipykernel_8156\\381660097.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_ids = torch.tensor(test_ids)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values,  dtype = torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abd8287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcec959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce6f5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "classifier = sentence_classifier(\n",
    "    n_vocab = n_vocab,\n",
    "    hidden_dim = hidden_dim,\n",
    "    embedding_dim = embedding_dim,\n",
    "    n_layers = n_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56070ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10ed7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226564c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
