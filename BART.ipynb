{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784f0c7f",
   "metadata": {},
   "source": [
    "## BART\n",
    "\n",
    "Bidirectional Auto-Regressive Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a48e55",
   "metadata": {},
   "source": [
    "Transformer based model\n",
    "\n",
    "BERT의 Encoder와 GPT의 디코더의 결합. Sequence to Sequence, Sep2sep 구조로 Denoising Autoencoder로 사전 학습."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395f73b",
   "metadata": {},
   "source": [
    "BERT 인코더는 입력 문장에서 일부 단어를 무작위로 마스킹해 처리, 마스킹된 단어를 맞추게 학습. BERT 인코더는 문장 전체 맥락을 이해하고 문맥 내 단어 간 상호작용을 파악.\n",
    "\n",
    "GPT는 언어 모델을 통해 문장의 이전 토큰을 입력으로 받고 다음에 올 토큰을 맞추도록 학습. 이를 통해 GPT는 문장 내 단어들의 순서와 문맥을 파악. 다음에 올 단어를 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfd163",
   "metadata": {},
   "source": [
    "BART는 사전 학습 시 노이즈 제거 오토인코더를 사용하므로, 입력 문장에 임의의 노이즈를 추가하고 원래 문장을 복원하도록 학습.\n",
    "\n",
    "노이즈가 추가된 텍스트를 인코더에 입력하고 원본 텍스트를 디코더에 입력해 디코더가 원본 텍스트를 생성 가능하게 학습하는 방식.\n",
    "\n",
    "이에 BART는 문장 구조와 의미를 보존하며 다양한 변형을 학습 가능함. 입력 문장에 제약 없이 노이즈 기법을 적용 가능하므로 더 풍부한 언어적 지식을 습득 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69ae8e",
   "metadata": {},
   "source": [
    "인코더를 사용하여 순방향 정보만 인식 가능한 GPT의 단점을 개선하여 양방향 문맥 정보를 반영, 디코더를 사용함으로써 문장 생성 분야에서 뛰어나지 않았던 BERT의 단점을 개선."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40577c",
   "metadata": {},
   "source": [
    "BERT vs BART\n",
    "\n",
    "BERT는 인코더만 사용. BART는 인코더와 디코더를 모두 사용. \n",
    "\n",
    "BART는 인코더와 디코더를 모두 사용하므로 트랜스포머와 유사한 구조를 가지나, tf에서는 인코더의 모든 계층과 디코더의 모든 계층 사이의 어텐션 연산을 수행한다면, BART는 인코더의 마지막 계층과 디코더의 각 계층 사이에서만 어텐션 연산을 수행."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293acd2",
   "metadata": {},
   "source": [
    "BART에서는 인코더의 마지막 계층과 디코더의 각 계층 사이에서만 어텐션 연산을 수행하므로, 정보 전달을 최소화하고 메모리 사용량을 줄일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c73b14",
   "metadata": {},
   "source": [
    "BART encoder에서는 입력 문장의 각 단어를 임베딩하고 복층 인코더를 거쳐 마지막 계층에서는 입력 문장 전체의 의미를 가장 잘 반영하는 벡터가 생성.\n",
    "\n",
    "이렇게 생성된 벡터는 디코더가 출력 문장을 생성할 때 참고됨. 디코더의 각 계층에서는 이전 계층에서 생성된 출력 문장의 정보를 활용하여 출력 문장을 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c896766",
   "metadata": {},
   "source": [
    "### 사전학습의 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9db609",
   "metadata": {},
   "source": [
    "BART encoder는 Bert의 마스킹된 언어 모델(MLM) 외에도 다양하게 노이즈 기법을 사용.\n",
    "\n",
    "토큰 마스킹 기법 외에도 토큰 삭제, 문장 교환, 문서 회전, 텍스트 채우기 등."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39dedd0",
   "metadata": {},
   "source": [
    "### 미세 조정의 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49631347",
   "metadata": {},
   "source": [
    "BART는 인코더와 디코더를 모두 사용. 미세 조정 시 각 다운스트림 작업에 맞게 입력 문장을 구성.\n",
    "\n",
    "즉, 인코더와 디코더에 다른 문장 구조로 입력."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27d642",
   "metadata": {},
   "source": [
    "BART는 tf 디코더를 사용하기 때문에 BERT가 해결하지 못했던 문장 생성 작업을 수행 가능.\n",
    "\n",
    "특히, 입력값을 조작하여 출력을 생성하는 추상적 질의응답(Abstractive Question Answering)과 문장 요약(Summarisation) 등의 작업에 적합."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c95735",
   "metadata": {},
   "source": [
    "BART는 이를 통해 문장의 의미를 파악하고, 새로운 문장을 생성하는 능력을 확보."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca8f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/dohyeong/miniconda3/Lib/site-packages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e778b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch import optim\n",
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184e09b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "news = load_dataset('argilla/news-summary', split='test')\n",
    "df = news.to_pandas().sample(5000, random_state=42)[['text', 'prediction']]\n",
    "df['prediction'] = df['prediction'].map(lambda x: x[0]['text'])\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6*len(df)), int(0.8*len(df))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5fc7cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source news: DANANG, Vietnam (Reuters) - Russian President Vladimir Putin said on Saturday he had a normal dialogue with U.S. leader Donald Trump at a summit in Vietnam, and described Trump as civil, well-educated, and comfortable to deal with. Putin said that a mooted bilateral sit-down meeting with Trump did not happen at the Asia-Pacific Economic Cooperation summit, citing scheduling issues on both sides and unspecified protocol issues. Putin, at a briefing for reporters at the end of the summit, said there was still a need for further U.S.-Russia contacts, both at the level of heads of state and their officials, to discuss issues including security and economic development.   \n"
     ]
    }
   ],
   "source": [
    "print(f'source news: {train.text.iloc[0][:2000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68e5853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5906087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, tokenizer, device):\n",
    "    tokenized = tokenizer(\n",
    "        text = data.text.tolist(),\n",
    "        padding = 'longest',\n",
    "        truncation = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "    labels = []\n",
    "    input_ids = tokenized['input_ids'].to(device)\n",
    "    attention_mask = tokenized['attention_mask'].to(device)\n",
    "    for target in data.prediction:\n",
    "        labels.append(tokenizer.encode(target, return_tensors = 'pt').squeeze())\n",
    "    labels = pad_sequence(labels, batch_first = True, padding_value = -100).to(device)\n",
    "    return TensorDataset(input_ids, attention_mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b35741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, sampler, batch_size):\n",
    "    data_sampler = sampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler = data_sampler, batch_size = batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21aaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# cuda out of memory 에러로 batch_size 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33c4c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ede64db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20417, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe04fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = 'facebook/bart-base'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7522d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_dataloader(train_dataset, RandomSampler, batch_size)\n",
    "\n",
    "valid_dataset = make_dataset(valid, tokenizer, device)\n",
    "valid_dataloader = get_dataloader(valid_dataset, RandomSampler, batch_size)\n",
    "\n",
    "test_dataset = make_dataset(test, tokenizer, device)\n",
    "test_dataloader = get_dataloader(test_dataset, RandomSampler, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4b0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path= 'facebook/bart-base'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7184b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729df72",
   "metadata": {},
   "source": [
    "### ROUGE(Recall-Oriented Understudy for Gisting Evaluation, ROUGT) score\n",
    "\n",
    "생성된 요약문과 정답 요약문이 얼마나 유사한지를 평가하기 위해 토큰의 N-gram 정밀도의 재현율을 이용해 평가하는 지표."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443faa59",
   "metadata": {},
   "source": [
    "#### ROUGE-L / ROUGE-LSUM / ROUGE-W\n",
    "\n",
    "ROUGE-L: 최장 공통부분 수열(Longest Common Subsequence, LCS) 기반의 통계\n",
    "\n",
    "ROUGE-LSUM: ROUGE-L의 변형. 텍스트 내의 개행 문자를 문장 경계로 인식, 각 문장쌍에 대해 LCS를 계산 후, union-LCS 값을 계산(각 문장 쌍의 LCS의 합집합).\n",
    "\n",
    "ROUGE-W: 가중치가 적용된 LCS. 연속된 LCS에 가중치를 부여하여 계산. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba1816",
   "metadata": {},
   "source": [
    "huggingface의 평가(evaluate) 라이브러리를 통해 루지 점수를 계산 가능(+ rouge_score 라이브러리 / Abseil 라이브러리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0581fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92d07be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge(preds, labels):\n",
    "    preds = preds.argmax(axis = -1)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens = True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "    \n",
    "    rouge2 = rouge_score.compute(\n",
    "        predictions = decoded_preds,\n",
    "        references = decoded_labels\n",
    "    )\n",
    "    return rouge2['rouge2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdb9e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for input_ids, attention_mask, labels in dataloader:\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    print('trained')\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1796eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss, val_rouge = 0.0, 0.0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            outputs = model(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, labels = labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "            rouge = calc_rouge(logits, label_ids)\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_rouge += rouge\n",
    "            \n",
    "    val_loss = val_loss / len(dataloader)\n",
    "    val_rouge = val_rouge / len(dataloader)\n",
    "    print('evaluated')\n",
    "    return val_loss, val_rouge\n",
    "    \n",
    "rouge_score = evaluate.load('rouge', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c88105ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8adc85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "989d7347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05:13:56'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "036a0660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05:14:41'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63c7d6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 05:15:19\n",
      "trained\n",
      "evaluated\n",
      "epoch: 1 train_loss: 2.1621 val_loss: 1.9281 val_ac: 0.2543\n",
      "done: 06:28:13\n",
      "model saved\n",
      "start: 06:28:18\n",
      "trained\n",
      "evaluated\n",
      "epoch: 2 train_loss: 1.5588 val_loss: 2.0158 val_ac: 0.2362\n",
      "done: 07:40:37\n",
      "start: 07:40:37\n",
      "trained\n",
      "evaluated\n",
      "epoch: 3 train_loss: 1.1519 val_loss: 2.0820 val_ac: 0.2478\n",
      "done: 08:52:13\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1000\n",
    "for epoch in range(epochs):\n",
    "    print('start:', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
    "    print(f'epoch: {epoch + 1} train_loss: {train_loss:.4f} val_loss: {val_loss:.4f} val_ac: {val_accuracy:.4f}')\n",
    "    print('done:', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), './bart_model.pt')\n",
    "        print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ab24a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bcf8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluated\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_rouge_score = evaluation(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a681c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 2.0445\n",
      "test_rouge2_score: 0.2552\n"
     ]
    }
   ],
   "source": [
    "print(f'test_loss: {test_loss:.4f}')\n",
    "print(f'test_rouge2_score: {test_rouge_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60e54514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e72a910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    task = 'summarization',\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 54,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9166e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: Clinton leads Trump by 4 points in Washington Post: ABC News poll\n",
      "model: Clinton leads Trump by 4 points in Washington Post-ABC News poll\n",
      "summary: Democrats question independence of Trump Supreme Court nominee\n",
      "model: Democrats raise concerns over Supreme Court nominee's independentness\n",
      "summary: In push for Yemen aid, U.S. warned Saudis of threats in Congress\n",
      "model: U.S. warns Saudi Arabia over Yemen crisis\n",
      "summary: Romanian ruling party leader investigated over 'criminal group'\n",
      "model: Romania opens probe into ruling Social Democrat Party leader over graft\n",
      "summary: Billionaire environmental activist Tom Steyer endorses Clinton\n",
      "model: Environmentalist Steyer backs Clinton for U.S. president\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    news_text = test.text.iloc[index]\n",
    "    summarisation = test.prediction.iloc[index]\n",
    "    predicted_summarisation = summarizer(news_text)[0]['summary_text']\n",
    "    print(f'summary: {summarisation}')\n",
    "    print(f'model: {predicted_summarisation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae4b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca579c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
