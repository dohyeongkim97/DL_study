{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc805bdc",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "입력 시퀀스를 양측에서 처리하여 이전과 이후의 단어를 모두 참조하며 단어의 의미를 파악.\n",
    "\n",
    "기존 모델들보다 정확하게 문맥을 파악하고 다양한 자연어 처리 작업에서 높은 성능을 보임.\n",
    "\n",
    "대규모 데이터를 통해 사전 학습되어 있으므로 전이 학습에 주로 활용. \n",
    "\n",
    "BERT는 일부나 전체를 다른 작업에서 재사용하여 적은 데이터로도 높은 정확도를 달성. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002f78f",
   "metadata": {},
   "source": [
    "### Masked Langauge Modeling, MLM\n",
    "\n",
    "입력 문장에서 임의로 일부 단어를 마스킹, 해당 단어를 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887bf5c",
   "metadata": {},
   "source": [
    "### Next Sentence Prediction, NSP\n",
    "\n",
    "두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장의 다음에 오는 문장인지의 여부 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658143af",
   "metadata": {},
   "source": [
    "BERT 모델의 토큰\n",
    "\n",
    "CLS / SEP / MASK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5de2d",
   "metadata": {},
   "source": [
    "CLS:\n",
    "\n",
    "입력 문장의 시작. 해당 토큰을 사용해 문장 분류 작업을 위한 정보를 제공.\n",
    "\n",
    "모델은 문장이 어떤 유형의 것인지 미리 파악 가능하게 됨. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c6607",
   "metadata": {},
   "source": [
    "SEP:\n",
    "\n",
    "입력 문장 내에서 문장의 구분을 위해 사용. 문장 분률 작업에서 복수 문장을 입력으로 받을 때, SEP 토큰을 통해 문장을 구분."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166d182",
   "metadata": {},
   "source": [
    "MASK:\n",
    "\n",
    "입력 문장 내에서 임의로 선택된 단어를 가리키는 토큰. 주어진 문장에서 단어를 가린 후 모델의 학습과 예측에 활용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3fe529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbf5c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "df = pd.DataFrame(corpus.test).sample(20000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d40688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = np.split(\n",
    "    df.sample(frac = 1, random_state = 42), [int(0.6*len(df)), int(0.8*len(df))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be297888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                     |   label |\n",
      "|------:|:---------------------------------------------------------|--------:|\n",
      "| 26891 | 역시 코믹액션은 성룡, 홍금보, 원표 삼인방이 최고지!!     |       1 |\n",
      "| 25024 | 점수 후하게 줘야것네 별 반개~                            |       0 |\n",
      "| 11666 | 오랜만에 느낄수 있는 [감독] 구타욕구.                    |       0 |\n",
      "| 40303 | 본지는 좀 됬지만 극장서 돈주고 본게 아직까지 아까운 영화 |       0 |\n",
      "| 18010 | 징키스칸이란 소재를 가지고 이것밖에 못만드냐             |       0 |\n"
     ]
    }
   ],
   "source": [
    "print(train.head(5).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2154331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\dohyeong\\\\Jupyter Python\\\\DL',\n",
       " '',\n",
       " 'c:\\\\users\\\\dohyeong\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\dohyeong\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:/Users/dohyeong/miniconda3/Lib/site-packages/')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch import optim\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch import nn\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d26fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, tokenizer, device):\n",
    "    tokenized = tokenizer(\n",
    "        text = data.text.tolist(),\n",
    "        padding= 'longest',\n",
    "        truncation = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "    input_ids = tokenized['input_ids'].to(device)\n",
    "    attention_mask = tokenized['attention_mask'].to(device)\n",
    "    labels = torch.tensor(data.label.values, dtype=torch.long).to(device)\n",
    "    return TensorDataset(input_ids, attention_mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df92fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, sampler, batch_size):\n",
    "    data_sampler = sampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler = data_sampler, batch_size = batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786fadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-multilingual-cased',\n",
    "    do_lower_case = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fea47f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670bcfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_dataloader(train_dataset, RandomSampler, batch_size)\n",
    "\n",
    "valid_dataset = make_dataset(valid, tokenizer, device)\n",
    "valid_dataloader = get_dataloader(valid_dataset, RandomSampler, batch_size)\n",
    "\n",
    "test_dataset = make_dataset(test, tokenizer, device)\n",
    "test_dataloader = get_dataloader(test_dataset, RandomSampler, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd55e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   101,  58466,   9812, 118956, 119122,  59095,  10892,   9434, 118888,\n",
      "           117,   9992,  40032,  30005,    117,   9612,  37824,   9410,  12030,\n",
      "         42337,  10739,  83491,  12508,    106,    106,    102,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0],\n",
      "       device='cuda:0'), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), tensor(1, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427fbee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-multilingual-cased',\n",
    "    num_labels = 2\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "169ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds.cpu().numpy(), axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4955eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for input_ids, attention_mask, labels in dataloader:\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc3d5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        val_loss, val_accuracy = 0.0, 0.0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                labels = labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            logtis = logits.detach().cpu().numpy()\n",
    "            labels_ids = labels.to('cpu').numpy()\n",
    "            accuracy = calc_accuracy(logits, labels_ids)\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_accuracy += accuracy\n",
    "            \n",
    "        val_loss = val_loss/len(dataloader)\n",
    "        val_accuracy = val_accuracy / len(dataloader)\n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb373e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train loss: 0.3377 val loss: 0.4308 val_acc: 0.8147\n",
      "Epoch: 2 train loss: 0.2713 val loss: 0.4399 val_acc: 0.8103\n",
      "Epoch: 3 train loss: 0.2086 val loss: 0.5041 val_acc: 0.8087\n",
      "Epoch: 4 train loss: 0.1629 val loss: 0.5177 val_acc: 0.8125\n",
      "Epoch: 5 train loss: 0.1245 val loss: 0.5745 val_acc: 0.8135\n"
     ]
    }
   ],
   "source": [
    "best_loss = 10000\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
    "    print(f'Epoch: {epoch+1} train loss: {train_loss:.4f} val loss: {val_loss:.4f} val_acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e4a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98bb7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f719397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea45323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275167d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
