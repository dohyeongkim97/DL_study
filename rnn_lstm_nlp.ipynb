{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd882df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Korpora\n",
    "from Korpora import Korpora\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d464030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea93370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : KakaoBrain\n",
      "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
      "    References :\n",
      "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
      "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
      "           (https://arxiv.org/abs/2004.03289)\n",
      "\n",
      "    This is the dataset repository for our paper\n",
      "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
      "    (https://arxiv.org/abs/2004.03289)\n",
      "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
      "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\multinli.train.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\snli_1.0_train.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\xnli.dev.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at C:\\Users\\dohyeong\\Korpora\\kornli\\xnli.test.ko.tsv\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('kornli')\n",
    "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba962b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [sentence.split() for sentence in corpus_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0937d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(\n",
    "    sentences = tokens,\n",
    "    vector_size = 128,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    sg = 1,\n",
    "    epochs = 3,\n",
    "    min_n = 2,\n",
    "    max_n = 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fd2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = '사랑해요'\n",
    "oov_vector = fasttext.wv[oov_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279e88b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[('사랑해', 0.911003828048706), ('사랑', 0.8714408874511719), ('사랑한', 0.8656883239746094), ('사랑해서', 0.8580443263053894), ('사랑해.', 0.8488666415214539)]\n"
     ]
    }
   ],
   "source": [
    "print(oov_token in fasttext.wv.index_to_key)\n",
    "print(fasttext.wv.most_similar(oov_vector, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ddeea",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b75a1e",
   "metadata": {},
   "source": [
    "```\n",
    "rnn = torch.nn.RNN(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers = 1,\n",
    "    nomlinearity='tanh',\n",
    "    bias=False,\n",
    "    batch_first = True,\n",
    "    dropout = 0,\n",
    "    bidirectional = False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71198de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c153a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128\n",
    "output_size = 256\n",
    "num_layers = 3\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf03d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.RNN(\n",
    "    input_size = input_size,\n",
    "    hidden_size = output_size,\n",
    "    num_layers = num_layers,\n",
    "    nonlinearity='tanh',\n",
    "    batch_first = True,\n",
    "    bidirectional=bidirectional,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31cd8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "sequence_len=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311ed9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(batch_size, sequence_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9580e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.rand(num_layers * (int(bidirectional)+1), batch_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93211fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hidden = model(inputs, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2479e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 512])\n",
      "torch.Size([6, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042a6f9",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f5ae6",
   "metadata": {},
   "source": [
    "Long Short Term Memory: RNN 모델이 갖던 기억력 부족과 Gradient Vanishing 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75119d49",
   "metadata": {},
   "source": [
    "RNN 모델은 장기 의존성 문제(Long Term Dependencies) 문제가 발생 가능. 활성화함수로 사용되는 tanh 함수나 ReLU 함수 특성으로 인해 역전파 과정에서 기울기 소실이나 폭주도 발생 가능함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a4e79",
   "metadata": {},
   "source": [
    "LSTM 모델은 순환 싱경망과 비슷한 구조를 가지나, Memory cell과 Gate 구조의 도입으로 상기한 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fd139",
   "metadata": {},
   "source": [
    "```\n",
    "lstm = torch.nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers=1,\n",
    "    bias=True,\n",
    "    batch_first=True,\n",
    "    dropout=0,\n",
    "    bidirectional=False,\n",
    "    proj_size=0\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd8b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d63103a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:879: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at ..\\aten\\src\\ATen\\native\\RNN.cpp:1493.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    }
   ],
   "source": [
    "input_size=128\n",
    "output_size=256\n",
    "num_layers = 3\n",
    "bidirectional=True\n",
    "proj_size=64\n",
    "\n",
    "model = nn.LSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional=bidirectional,\n",
    "    proj_size=proj_size\n",
    ")\n",
    "\n",
    "batch_size=4\n",
    "sequence_len=6\n",
    "\n",
    "inputs=torch.randn(batch_size, sequence_len, input_size)\n",
    "h0=torch.rand(\n",
    "    num_layers * (int(bidirectional)+1),\n",
    "    batch_size,\n",
    "    proj_size if proj_size > 0 else output_size,\n",
    ")\n",
    "c0 = torch.rand(num_layers * (int(bidirectional)+1), batch_size, output_size)\n",
    "\n",
    "outputs, (hn, cn) = model(inputs, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c775b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 128])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf149422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4, 64])\n",
      "torch.Size([6, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(hn.shape)\n",
    "print(cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f03911",
   "metadata": {},
   "source": [
    "# P/N classification model by using RNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afffa8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type='lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx = 0\n",
    "        )\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f611cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a782861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\dohyeong\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e09a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c6105a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "train_size: 45000\n",
      "test_size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(train.head(5).to_markdown())\n",
    "print(f'train_size: {len(train)}')\n",
    "print(f'test_size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d38f7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "556f770f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okt done\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "        vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "tokenizer = Okt()\n",
    "print('okt done')\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=['<pad>', '<unk>'])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f72abe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436279f1",
   "metadata": {},
   "source": [
    "# int encoding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bf2888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62361398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "486afa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = token_to_id['<unk>']\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edbd6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e6d00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b331ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values,  dtype = torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0836473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "282241f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c6c22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "classifier = sentence_classifier(\n",
    "    n_vocab = n_vocab,\n",
    "    hidden_dim = hidden_dim,\n",
    "    embedding_dim = embedding_dim,\n",
    "    n_layers = n_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84212807",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "186727de",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb7104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8c455b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4ed1ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datasets, criterion, device, optimizer, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % interval == 0:\n",
    "            print(f'train_loss {step}: {np.mean(losses)}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "64d3a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    corrects = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (input_ids, labels) in enumerate(datasets):\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            predictions = (logits > 0.5).float()\n",
    "            correct = (predictions == labels).float().sum()\n",
    "            corrects.append(correct)\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    accuracy = sum(corrects) / len(datasets.dataset)\n",
    "    print(f'Test Loss: {avg_loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3c045205",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "04f40e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ea293675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(128, 256, proj_size=64, num_layers=3, batch_first=True, bidirectional=True)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a957893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_classifier(\n",
       "  (embedding): Embedding(5002, 128)\n",
       "  (model): LSTM(128, 64, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (classifier): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f68185f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0: 0.6870390772819519\n",
      "train_loss 500: 0.6931035059416841\n",
      "train_loss 1000: 0.6931176014713474\n",
      "train_loss 1500: 0.6932658474894859\n",
      "train_loss 2000: 0.6932994445701172\n",
      "train_loss 2500: 0.6933506678124038\n",
      "Test Loss: 0.693649560308304, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.7011062502861023\n",
      "train_loss 500: 0.6938849688766008\n",
      "train_loss 1000: 0.6936074681096263\n",
      "train_loss 1500: 0.6934359823759996\n",
      "train_loss 2000: 0.693416163779568\n",
      "train_loss 2500: 0.69341177191557\n",
      "Test Loss: 0.6936506246225521, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.7008789777755737\n",
      "train_loss 500: 0.6933555244923589\n",
      "train_loss 1000: 0.6934144872051853\n",
      "train_loss 1500: 0.6936174673211328\n",
      "train_loss 2000: 0.6935650970505691\n",
      "train_loss 2500: 0.6935454028122714\n",
      "Test Loss: 0.6936476064947086, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.6898178458213806\n",
      "train_loss 500: 0.6935004901267335\n",
      "train_loss 1000: 0.6934609407192462\n",
      "train_loss 1500: 0.6933279882106361\n",
      "train_loss 2000: 0.6932223185725596\n",
      "train_loss 2500: 0.6932714487828526\n",
      "Test Loss: 0.6936517239759524, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.6943269968032837\n",
      "train_loss 500: 0.6936947791400307\n",
      "train_loss 1000: 0.6936011469209349\n",
      "train_loss 1500: 0.6934918955037945\n",
      "train_loss 2000: 0.6934469353550974\n",
      "train_loss 2500: 0.6935090088262791\n",
      "Test Loss: 0.6936463483225423, Test Accuracy: 0.4821999967098236\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, device, optimizer, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "75127903",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f68bdca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "80f44925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [ 0.03330366 -0.3556527   0.01270325 -0.20829733  0.13543186 -0.18150993\n",
      "  0.02951619 -0.09317388 -0.18373103  0.16290817 -0.05611473 -0.03719699\n",
      "  0.02471198 -0.16446424  0.20844965  0.365992    0.07688164  0.01230337\n",
      " -0.10401959  0.2306495  -0.05232944  0.24145994 -0.1746113  -0.37058467\n",
      " -0.05984326  0.1193573  -0.0958266   0.29869407  0.21386899 -0.33867097\n",
      " -0.08592527  0.14896092 -0.04156579  0.20819351 -0.07515699  0.36699715\n",
      "  0.22735111 -0.20565431 -0.00274597 -0.01631356 -0.04651385  0.25305504\n",
      " -0.03152147 -0.19845273  0.37523302  0.05237744 -0.07598199 -0.06152836\n",
      "  0.06555579 -0.04907126  0.08199442 -0.09313392  0.20899668  0.01285784\n",
      " -0.02268853 -0.07997185  0.3502871   0.13160667 -0.074122   -0.15173548\n",
      "  0.4365486   0.0189922   0.14980258 -0.04991355  0.23081478 -0.11140083\n",
      "  0.19742823  0.09540525 -0.17929368 -0.10804195  0.04769334 -0.10192904\n",
      " -0.45491502 -0.19848992  0.25304458 -0.22504833  0.01785926 -0.06307909\n",
      " -0.12844144  0.39623684 -0.12601593 -0.14821033 -0.13722485  0.5728623\n",
      "  0.17312016  0.44024113  0.2123206  -0.2600878   0.16275486  0.16401431\n",
      "  0.02745119 -0.09396034  0.47935465 -0.1368422   0.16269538  0.18395665\n",
      " -0.3319633  -0.21353772 -0.08830211 -0.21254921 -0.32933995 -0.04361098\n",
      " -0.06212078  0.01416937  0.1033061  -0.13764854 -0.10583091  0.00155733\n",
      "  0.3436335   0.01453656 -0.05291831 -0.21598019  0.04882742  0.02692439\n",
      " -0.46320444 -0.20639098  0.19935212  0.17366989 -0.09630223  0.04738799\n",
      " -0.29930973 -0.16033767 -0.2646261   0.03289291 -0.3418703  -0.01632837\n",
      " -0.25246686  0.00679886]\n"
     ]
    }
   ],
   "source": [
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "df5a7499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_to_embedding['보고싶다'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2c70d42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12029147"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_embedding['보고싶다'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "300e960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7f8e9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./word2vec.model')\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a481d874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c404d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5002"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(init_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5927bb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<unk>',\n",
       " 2: '.',\n",
       " 3: '이',\n",
       " 4: '영화',\n",
       " 5: '의',\n",
       " 6: '..',\n",
       " 7: '가',\n",
       " 8: '에',\n",
       " 9: '...',\n",
       " 10: '을',\n",
       " 11: '도',\n",
       " 12: '들',\n",
       " 13: ',',\n",
       " 14: '는',\n",
       " 15: '를',\n",
       " 16: '은',\n",
       " 17: '?',\n",
       " 18: '너무',\n",
       " 19: '한',\n",
       " 20: '다',\n",
       " 21: '정말',\n",
       " 22: '만',\n",
       " 23: '진짜',\n",
       " 24: '적',\n",
       " 25: '!',\n",
       " 26: '로',\n",
       " 27: '점',\n",
       " 28: '으로',\n",
       " 29: '에서',\n",
       " 30: '연기',\n",
       " 31: '평점',\n",
       " 32: '과',\n",
       " 33: '것',\n",
       " 34: '~',\n",
       " 35: '최고',\n",
       " 36: '내',\n",
       " 37: '그',\n",
       " 38: '나',\n",
       " 39: '안',\n",
       " 40: '잘',\n",
       " 41: '와',\n",
       " 42: '인',\n",
       " 43: '생각',\n",
       " 44: '게',\n",
       " 45: '못',\n",
       " 46: '이런',\n",
       " 47: '왜',\n",
       " 48: '....',\n",
       " 49: '스토리',\n",
       " 50: '드라마',\n",
       " 51: '사람',\n",
       " 52: '이다',\n",
       " 53: '감동',\n",
       " 54: '1',\n",
       " 55: '보고',\n",
       " 56: '하는',\n",
       " 57: '때',\n",
       " 58: '더',\n",
       " 59: '하고',\n",
       " 60: '고',\n",
       " 61: '아',\n",
       " 62: '말',\n",
       " 63: '감독',\n",
       " 64: 'ㅋㅋ',\n",
       " 65: '그냥',\n",
       " 66: '배우',\n",
       " 67: '내용',\n",
       " 68: '거',\n",
       " 69: '중',\n",
       " 70: '재미',\n",
       " 71: '까지',\n",
       " 72: '본',\n",
       " 73: '보다',\n",
       " 74: '요',\n",
       " 75: '!!',\n",
       " 76: '없는',\n",
       " 77: '좀',\n",
       " 78: '뭐',\n",
       " 79: '시간',\n",
       " 80: '지',\n",
       " 81: '수',\n",
       " 82: '쓰레기',\n",
       " 83: '사랑',\n",
       " 84: '봤는데',\n",
       " 85: '볼',\n",
       " 86: '네',\n",
       " 87: '작품',\n",
       " 88: '다시',\n",
       " 89: '하나',\n",
       " 90: '10',\n",
       " 91: '없다',\n",
       " 92: '할',\n",
       " 93: '이건',\n",
       " 94: '마지막',\n",
       " 95: '2',\n",
       " 96: '저',\n",
       " 97: '같은',\n",
       " 98: '정도',\n",
       " 99: 'ㅠㅠ',\n",
       " 100: '있는',\n",
       " 101: '완전',\n",
       " 102: '좋은',\n",
       " 103: 'ㅋ',\n",
       " 104: '대',\n",
       " 105: '처음',\n",
       " 106: '주인공',\n",
       " 107: '입니다',\n",
       " 108: 'ㅋㅋㅋ',\n",
       " 109: '장면',\n",
       " 110: '액션',\n",
       " 111: '이렇게',\n",
       " 112: '최악',\n",
       " 113: '지금',\n",
       " 114: '보는',\n",
       " 115: '걸',\n",
       " 116: '이야기',\n",
       " 117: '하',\n",
       " 118: '개',\n",
       " 119: '임',\n",
       " 120: '3',\n",
       " 121: '없고',\n",
       " 122: '별로',\n",
       " 123: '끝',\n",
       " 124: '연출',\n",
       " 125: '참',\n",
       " 126: '돈',\n",
       " 127: \"'\",\n",
       " 128: '서',\n",
       " 129: '느낌',\n",
       " 130: '봐도',\n",
       " 131: '듯',\n",
       " 132: '별',\n",
       " 133: '라',\n",
       " 134: '기',\n",
       " 135: '인데',\n",
       " 136: 'ㅡㅡ',\n",
       " 137: '역시',\n",
       " 138: '명작',\n",
       " 139: '이해',\n",
       " 140: '많이',\n",
       " 141: '난',\n",
       " 142: '재밌게',\n",
       " 143: '라고',\n",
       " 144: '면',\n",
       " 145: '^^',\n",
       " 146: '때문',\n",
       " 147: '그리고',\n",
       " 148: '이영화',\n",
       " 149: '여자',\n",
       " 150: '보면',\n",
       " 151: '또',\n",
       " 152: '두',\n",
       " 153: '전',\n",
       " 154: '인생',\n",
       " 155: '해서',\n",
       " 156: '!!!',\n",
       " 157: '에게',\n",
       " 158: '편',\n",
       " 159: '꼭',\n",
       " 160: '이나',\n",
       " 161: '보기',\n",
       " 162: '성',\n",
       " 163: '아깝다',\n",
       " 164: '된',\n",
       " 165: '부터',\n",
       " 166: '짱',\n",
       " 167: '여',\n",
       " 168: '애',\n",
       " 169: '무슨',\n",
       " 170: '이고',\n",
       " 171: '같다',\n",
       " 172: '마음',\n",
       " 173: '일',\n",
       " 174: '수준',\n",
       " 175: '랑',\n",
       " 176: '님',\n",
       " 177: '기억',\n",
       " 178: '결말',\n",
       " 179: '엔',\n",
       " 180: '야',\n",
       " 181: 'ㅎㅎ',\n",
       " 182: '제',\n",
       " 183: ';;',\n",
       " 184: '영',\n",
       " 185: '해',\n",
       " 186: '넘',\n",
       " 187: '한다',\n",
       " 188: '라는',\n",
       " 189: 'ㅠ',\n",
       " 190: '함',\n",
       " 191: '한번',\n",
       " 192: '현실',\n",
       " 193: '가슴',\n",
       " 194: '하게',\n",
       " 195: '반전',\n",
       " 196: '한국',\n",
       " 197: '없이',\n",
       " 198: '알',\n",
       " 199: '매력',\n",
       " 200: '??',\n",
       " 201: '소재',\n",
       " 202: '하지만',\n",
       " 203: '~~',\n",
       " 204: '번',\n",
       " 205: '속',\n",
       " 206: '남자',\n",
       " 207: '인간',\n",
       " 208: '냐',\n",
       " 209: '아이',\n",
       " 210: '전개',\n",
       " 211: '씨',\n",
       " 212: '되는',\n",
       " 213: '추천',\n",
       " 214: '우리',\n",
       " 215: '만든',\n",
       " 216: '뿐',\n",
       " 217: '자체',\n",
       " 218: '눈물',\n",
       " 219: '급',\n",
       " 220: '니',\n",
       " 221: '봤다',\n",
       " 222: '화',\n",
       " 223: '모든',\n",
       " 224: '다른',\n",
       " 225: '모습',\n",
       " 226: '가장',\n",
       " 227: '음악',\n",
       " 228: '않고',\n",
       " 229: '하지',\n",
       " 230: '합니다',\n",
       " 231: '계속',\n",
       " 232: '모두',\n",
       " 233: '인지',\n",
       " 234: '코미디',\n",
       " 235: '줄',\n",
       " 236: '인가',\n",
       " 237: '좋다',\n",
       " 238: '그래도',\n",
       " 239: '지만',\n",
       " 240: '캐릭터',\n",
       " 241: '실망',\n",
       " 242: '대한',\n",
       " 243: '이제',\n",
       " 244: '에는',\n",
       " 245: '원작',\n",
       " 246: '연기력',\n",
       " 247: '솔직히',\n",
       " 248: '분',\n",
       " 249: '시리즈',\n",
       " 250: '자',\n",
       " 251: 'ㅋㅋㅋㅋ',\n",
       " 252: '눈',\n",
       " 253: '진심',\n",
       " 254: '있다',\n",
       " 255: '개봉',\n",
       " 256: '뭔가',\n",
       " 257: ';',\n",
       " 258: '하면',\n",
       " 259: '처럼',\n",
       " 260: '대박',\n",
       " 261: '했다',\n",
       " 262: '중간',\n",
       " 263: '이상',\n",
       " 264: '여운',\n",
       " 265: 'OO',\n",
       " 266: '였다',\n",
       " 267: '부분',\n",
       " 268: '표현',\n",
       " 269: '\"\"\"\"',\n",
       " 270: '4',\n",
       " 271: '기대',\n",
       " 272: '공감',\n",
       " 273: ',,',\n",
       " 274: '일본',\n",
       " 275: '굿',\n",
       " 276: '이네',\n",
       " 277: '이라',\n",
       " 278: '가족',\n",
       " 279: '근데',\n",
       " 280: '나오는',\n",
       " 281: '(',\n",
       " 282: '재미없다',\n",
       " 283: '전혀',\n",
       " 284: '보지',\n",
       " 285: '기분',\n",
       " 286: '재밌다',\n",
       " 287: '아니라',\n",
       " 288: '어떻게',\n",
       " 289: '그런',\n",
       " 290: '아주',\n",
       " 291: '허',\n",
       " 292: '많은',\n",
       " 293: '아니고',\n",
       " 294: 'ㅎ',\n",
       " 295: '내내',\n",
       " 296: '이랑',\n",
       " 297: '없음',\n",
       " 298: 'ㅜㅜ',\n",
       " 299: '건지',\n",
       " 300: '제목',\n",
       " 301: '영상',\n",
       " 302: '뭔',\n",
       " 303: '아닌',\n",
       " 304: '대사',\n",
       " 305: '이라는',\n",
       " 306: '아니다',\n",
       " 307: '작',\n",
       " 308: '애니',\n",
       " 309: '좋고',\n",
       " 310: '공포',\n",
       " 311: '요즘',\n",
       " 312: '움',\n",
       " 313: '후',\n",
       " 314: '하다',\n",
       " 315: '-',\n",
       " 316: '0',\n",
       " 317: '했는데',\n",
       " 318: '봤습니다',\n",
       " 319: '재미있게',\n",
       " 320: '극장',\n",
       " 321: '놈',\n",
       " 322: '점도',\n",
       " 323: '용',\n",
       " 324: '특히',\n",
       " 325: '조금',\n",
       " 326: '밖에',\n",
       " 327: ')',\n",
       " 328: '감',\n",
       " 329: '건',\n",
       " 330: '몰입',\n",
       " 331: '삶',\n",
       " 332: '긴장감',\n",
       " 333: '보면서',\n",
       " 334: '작가',\n",
       " 335: '않는',\n",
       " 336: '연',\n",
       " 337: '나름',\n",
       " 338: '명',\n",
       " 339: '노래',\n",
       " 340: '이란',\n",
       " 341: '차라리',\n",
       " 342: '\"\"\"',\n",
       " 343: '딱',\n",
       " 344: '우리나라',\n",
       " 345: '잼',\n",
       " 346: '욕',\n",
       " 347: '이지',\n",
       " 348: '당시',\n",
       " 349: '보다가',\n",
       " 350: '봐',\n",
       " 351: '한테',\n",
       " 352: '의미',\n",
       " 353: '위',\n",
       " 354: '시작',\n",
       " 355: '노잼',\n",
       " 356: '친구',\n",
       " 357: '않은',\n",
       " 358: '오랜',\n",
       " 359: '보게',\n",
       " 360: '아름다운',\n",
       " 361: '영화로',\n",
       " 362: '봄',\n",
       " 363: '스릴러',\n",
       " 364: '아까운',\n",
       " 365: '개인',\n",
       " 366: '.....',\n",
       " 367: '재밌어요',\n",
       " 368: '제일',\n",
       " 369: '공포영화',\n",
       " 370: '만드는',\n",
       " 371: '력',\n",
       " 372: '이라고',\n",
       " 373: '에도',\n",
       " 374: '몇',\n",
       " 375: '시',\n",
       " 376: '이유',\n",
       " 377: '데',\n",
       " 378: '뻔한',\n",
       " 379: '된다',\n",
       " 380: '해도',\n",
       " 381: '세상',\n",
       " 382: '뭘',\n",
       " 383: '웃음',\n",
       " 384: '알바',\n",
       " 385: '아직도',\n",
       " 386: '점수',\n",
       " 387: '아무',\n",
       " 388: '엄청',\n",
       " 389: '보니',\n",
       " 390: '오',\n",
       " 391: '식',\n",
       " 392: '시나리오',\n",
       " 393: '보세요',\n",
       " 394: '5',\n",
       " 395: '준',\n",
       " 396: '좋아하는',\n",
       " 397: '막장',\n",
       " 398: '제대로',\n",
       " 399: '있고',\n",
       " 400: '♥',\n",
       " 401: '재밌음',\n",
       " 402: '초반',\n",
       " 403: '8',\n",
       " 404: '무엇',\n",
       " 405: '결국',\n",
       " 406: '어',\n",
       " 407: '최고다',\n",
       " 408: '싶다',\n",
       " 409: '앞',\n",
       " 410: '나도',\n",
       " 411: '출연',\n",
       " 412: '미국',\n",
       " 413: '그렇게',\n",
       " 414: '설정',\n",
       " 415: '든',\n",
       " 416: '한국영',\n",
       " 417: '시대',\n",
       " 418: 'ㅡ',\n",
       " 419: '절대',\n",
       " 420: '너',\n",
       " 421: '위해',\n",
       " 422: '같이',\n",
       " 423: '구',\n",
       " 424: '극',\n",
       " 425: '봤어요',\n",
       " 426: '도대체',\n",
       " 427: '관객',\n",
       " 428: '제발',\n",
       " 429: '대해',\n",
       " 430: '보는내내',\n",
       " 431: '물',\n",
       " 432: '남',\n",
       " 433: '사실',\n",
       " 434: '자신',\n",
       " 435: '7',\n",
       " 436: '~~~',\n",
       " 437: '살',\n",
       " 438: '니까',\n",
       " 439: '분위기',\n",
       " 440: '정신',\n",
       " 441: '마다',\n",
       " 442: '류',\n",
       " 443: '걍',\n",
       " 444: '인상',\n",
       " 445: '같아요',\n",
       " 446: '더빙',\n",
       " 447: '졸작',\n",
       " 448: '신',\n",
       " 449: '주고',\n",
       " 450: '이딴',\n",
       " 451: '후반',\n",
       " 452: '준다',\n",
       " 453: '소리',\n",
       " 454: '멋진',\n",
       " 455: '영화관',\n",
       " 456: '뒤',\n",
       " 457: '좋았다',\n",
       " 458: '어릴',\n",
       " 459: '가지',\n",
       " 460: '남는',\n",
       " 461: '이야',\n",
       " 462: '주는',\n",
       " 463: '장난',\n",
       " 464: '봐서',\n",
       " 465: '엄마',\n",
       " 466: '봤던',\n",
       " 467: '좋아요',\n",
       " 468: '지루하고',\n",
       " 469: '수작',\n",
       " 470: '엔딩',\n",
       " 471: 'OOO',\n",
       " 472: '안되는',\n",
       " 473: '라면',\n",
       " 474: '그저',\n",
       " 475: '갈수록',\n",
       " 476: '추억',\n",
       " 477: '이름',\n",
       " 478: '여기',\n",
       " 479: '충격',\n",
       " 480: '오늘',\n",
       " 481: '아무리',\n",
       " 482: '킬링타임',\n",
       " 483: '개연',\n",
       " 484: '될',\n",
       " 485: '코믹',\n",
       " 486: '세',\n",
       " 487: '주',\n",
       " 488: '포스터',\n",
       " 489: '함께',\n",
       " 490: '반',\n",
       " 491: '간',\n",
       " 492: '지루함',\n",
       " 493: '감정',\n",
       " 494: '전쟁',\n",
       " 495: '문제',\n",
       " 496: '접',\n",
       " 497: '싶은',\n",
       " 498: '큰',\n",
       " 499: '모르겠다',\n",
       " 500: '얘기',\n",
       " 501: '옛날',\n",
       " 502: '스릴',\n",
       " 503: '평가',\n",
       " 504: '강추',\n",
       " 505: '얼굴',\n",
       " 506: '이지만',\n",
       " 507: '역사',\n",
       " 508: '까',\n",
       " 509: '소름',\n",
       " 510: '팬',\n",
       " 511: '다큐',\n",
       " 512: '어떤',\n",
       " 513: '예술',\n",
       " 514: '울',\n",
       " 515: '조',\n",
       " 516: '아니',\n",
       " 517: '!!!!',\n",
       " 518: '엉',\n",
       " 519: '하면서',\n",
       " 520: ',,,',\n",
       " 521: '비',\n",
       " 522: '구성',\n",
       " 523: '머',\n",
       " 524: '지루한',\n",
       " 525: '맘',\n",
       " 526: '애니메이션',\n",
       " 527: '기도',\n",
       " 528: '않다',\n",
       " 529: '그것',\n",
       " 530: '등',\n",
       " 531: '한마디',\n",
       " 532: '대체',\n",
       " 533: '했던',\n",
       " 534: '그래서',\n",
       " 535: '머리',\n",
       " 536: '캐스팅',\n",
       " 537: '분들',\n",
       " 538: '하는데',\n",
       " 539: '괜찮은',\n",
       " 540: '티비',\n",
       " 541: '후회',\n",
       " 542: '얼마나',\n",
       " 543: '보단',\n",
       " 544: '배경',\n",
       " 545: '상황',\n",
       " 546: '아직',\n",
       " 547: '않는다',\n",
       " 548: '질',\n",
       " 549: 'ㅜ',\n",
       " 550: '집',\n",
       " 551: '짜증',\n",
       " 552: '음',\n",
       " 553: '주연',\n",
       " 554: '걸작',\n",
       " 555: '했지만',\n",
       " 556: '+',\n",
       " 557: '다운',\n",
       " 558: '영화인',\n",
       " 559: '상',\n",
       " 560: '만화',\n",
       " 561: '건가',\n",
       " 562: 'ㅉㅉ',\n",
       " 563: '재밌는',\n",
       " 564: '6',\n",
       " 565: '첨',\n",
       " 566: '총',\n",
       " 567: '누구',\n",
       " 568: '글',\n",
       " 569: '같은데',\n",
       " 570: '보는데',\n",
       " 571: '이후',\n",
       " 572: '집중',\n",
       " 573: '혼자',\n",
       " 574: '시절',\n",
       " 575: '훨씬',\n",
       " 576: '만들어',\n",
       " 577: '장르',\n",
       " 578: '재미없음',\n",
       " 579: '언제',\n",
       " 580: '비교',\n",
       " 581: '마세요',\n",
       " 582: '방송',\n",
       " 583: '지루하다',\n",
       " 584: '나온',\n",
       " 585: '편이',\n",
       " 586: '동안',\n",
       " 587: '~!',\n",
       " 588: '/',\n",
       " 589: '바로',\n",
       " 590: '그나마',\n",
       " 591: '원',\n",
       " 592: '없었다',\n",
       " 593: '실화',\n",
       " 594: '웃기',\n",
       " 595: '봤지만',\n",
       " 596: '봐라',\n",
       " 597: '없네',\n",
       " 598: '아들',\n",
       " 599: '인물',\n",
       " 600: '부',\n",
       " 601: '9',\n",
       " 602: '너무나',\n",
       " 603: '초딩',\n",
       " 604: '???',\n",
       " 605: '진',\n",
       " 606: '어느',\n",
       " 607: '봤음',\n",
       " 608: '책',\n",
       " 609: '할수',\n",
       " 610: '억지',\n",
       " 611: '이었다',\n",
       " 612: '몰입도',\n",
       " 613: '굳',\n",
       " 614: '씬',\n",
       " 615: '먹',\n",
       " 616: '같음',\n",
       " 617: '시즌',\n",
       " 618: '약간',\n",
       " 619: '됨',\n",
       " 620: '만점',\n",
       " 621: '자기',\n",
       " 622: '에요',\n",
       " 623: '라도',\n",
       " 624: '코',\n",
       " 625: '날',\n",
       " 626: 'B',\n",
       " 627: '네이버',\n",
       " 628: '발',\n",
       " 629: '휴',\n",
       " 630: '어디',\n",
       " 631: '당신',\n",
       " 632: '꿈',\n",
       " 633: '쳐',\n",
       " 634: '본다',\n",
       " 635: '나와서',\n",
       " 636: '비디오',\n",
       " 637: '꽤',\n",
       " 638: '나라',\n",
       " 639: '매우',\n",
       " 640: '보',\n",
       " 641: '목소리',\n",
       " 642: '주제',\n",
       " 643: '저런',\n",
       " 644: '이리',\n",
       " 645: '맛',\n",
       " 646: '아님',\n",
       " 647: '누가',\n",
       " 648: '존나',\n",
       " 649: '나이',\n",
       " 650: '그대로',\n",
       " 651: '사회',\n",
       " 652: '보다는',\n",
       " 653: '소설',\n",
       " 654: '가치',\n",
       " 655: '답',\n",
       " 656: '빨리',\n",
       " 657: '였습니다',\n",
       " 658: '하기',\n",
       " 659: '쯤',\n",
       " 660: '없어',\n",
       " 661: '죽',\n",
       " 662: '다음',\n",
       " 663: '해야',\n",
       " 664: '재밌고',\n",
       " 665: '만하',\n",
       " 666: '재미있어요',\n",
       " 667: '상당히',\n",
       " 668: '전체',\n",
       " 669: '바',\n",
       " 670: '이하',\n",
       " 671: '발연기',\n",
       " 672: '굉장히',\n",
       " 673: '안보',\n",
       " 674: '히',\n",
       " 675: '힘',\n",
       " 676: '여배우',\n",
       " 677: '판',\n",
       " 678: '성룡',\n",
       " 679: '로맨스',\n",
       " 680: '질질',\n",
       " 681: ';;;',\n",
       " 682: '평론가',\n",
       " 683: 'CG',\n",
       " 684: '가는',\n",
       " 685: '하네요',\n",
       " 686: '같네요',\n",
       " 687: '좀비',\n",
       " 688: '좋아',\n",
       " 689: '어설픈',\n",
       " 690: '죠',\n",
       " 691: '간만',\n",
       " 692: '아버지',\n",
       " 693: '실제',\n",
       " 694: '감성',\n",
       " 695: '그때',\n",
       " 696: '예전',\n",
       " 697: '라서',\n",
       " 698: '거의',\n",
       " 699: '첫',\n",
       " 700: '낭비',\n",
       " 701: '아까',\n",
       " 702: '삼류',\n",
       " 703: 'TV',\n",
       " 704: '어렸을',\n",
       " 705: '어른',\n",
       " 706: '세계',\n",
       " 707: '년',\n",
       " 708: '보여주는',\n",
       " 709: '짜리',\n",
       " 710: '있지만',\n",
       " 711: '깊은',\n",
       " 712: '점주',\n",
       " 713: '간다',\n",
       " 714: '재',\n",
       " 715: '모',\n",
       " 716: '잔잔한',\n",
       " 717: '둘',\n",
       " 718: '선',\n",
       " 719: '없어서',\n",
       " 720: '어린',\n",
       " 721: '사',\n",
       " 722: '해주는',\n",
       " 723: '이라도',\n",
       " 724: '여서',\n",
       " 725: '슬픈',\n",
       " 726: '란',\n",
       " 727: '판타지',\n",
       " 728: '여주',\n",
       " 729: '전부',\n",
       " 730: '는데',\n",
       " 731: '몇번',\n",
       " 732: '재밋',\n",
       " 733: '극장판',\n",
       " 734: '난다',\n",
       " 735: '좋음',\n",
       " 736: '재미있다',\n",
       " 737: '올',\n",
       " 738: 'ㅎㅎㅎ',\n",
       " 739: '되고',\n",
       " 740: '이에요',\n",
       " 741: '터',\n",
       " 742: '요소',\n",
       " 743: '낫다',\n",
       " 744: '소',\n",
       " 745: '미친',\n",
       " 746: '진정한',\n",
       " 747: '전달',\n",
       " 748: '장',\n",
       " 749: '열',\n",
       " 750: '하네',\n",
       " 751: '거지',\n",
       " 752: '교훈',\n",
       " 753: '재밌네요',\n",
       " 754: '버린',\n",
       " 755: '따뜻한',\n",
       " 756: '순간',\n",
       " 757: '그만',\n",
       " 758: '상상',\n",
       " 759: '개그',\n",
       " 760: '낸',\n",
       " 761: '중국',\n",
       " 762: '밑',\n",
       " 763: '그런지',\n",
       " 764: '10년',\n",
       " 765: '으로도',\n",
       " 766: '믿고',\n",
       " 767: '시청률',\n",
       " 768: '각본',\n",
       " 769: '존재',\n",
       " 770: '현',\n",
       " 771: '곳',\n",
       " 772: '사건',\n",
       " 773: '이냐',\n",
       " 774: '아까워',\n",
       " 775: '좋은데',\n",
       " 776: '에서도',\n",
       " 777: '라니',\n",
       " 778: '그러나',\n",
       " 779: '화보',\n",
       " 780: '에선',\n",
       " 781: '환상',\n",
       " 782: '볼때',\n",
       " 783: '암',\n",
       " 784: '아닌가',\n",
       " 785: '있어',\n",
       " 786: '멜로',\n",
       " 787: '20',\n",
       " 788: '좋겠다',\n",
       " 789: '줄거리',\n",
       " 790: '말고',\n",
       " 791: '거기',\n",
       " 792: '되어',\n",
       " 793: '였음',\n",
       " 794: '인거',\n",
       " 795: '병맛',\n",
       " 796: '점점',\n",
       " 797: '나쁜',\n",
       " 798: '더럽게',\n",
       " 799: '무',\n",
       " 800: '감상',\n",
       " 801: '항상',\n",
       " 802: 'tv',\n",
       " 803: '온',\n",
       " 804: '가서',\n",
       " 805: 'ㄷㄷ',\n",
       " 806: '그림',\n",
       " 807: '대한민국',\n",
       " 808: '피',\n",
       " 809: '화면',\n",
       " 810: '군',\n",
       " 811: '아쉽다',\n",
       " 812: '에서는',\n",
       " 813: '대로',\n",
       " 814: '흥행',\n",
       " 815: '선택',\n",
       " 816: '여러',\n",
       " 817: '새끼',\n",
       " 818: 'ㅋㅋㅋㅋㅋ',\n",
       " 819: '좋았어요',\n",
       " 820: '형',\n",
       " 821: '초',\n",
       " 822: '완성',\n",
       " 823: '봐야',\n",
       " 824: '프랑스',\n",
       " 825: '이번',\n",
       " 826: '있는데',\n",
       " 827: '잘만',\n",
       " 828: '상영',\n",
       " 829: '또한',\n",
       " 830: '치고',\n",
       " 831: '성우',\n",
       " 832: '충분히',\n",
       " 833: '만이',\n",
       " 834: 'ㅠㅠㅠ',\n",
       " 835: '햇',\n",
       " 836: '......',\n",
       " 837: '헐',\n",
       " 838: '땜',\n",
       " 839: '갑자기',\n",
       " 840: '좋네요',\n",
       " 841: '단',\n",
       " 842: '제작',\n",
       " 843: '나온다',\n",
       " 844: '리',\n",
       " 845: '몸',\n",
       " 846: '의도',\n",
       " 847: '그녀',\n",
       " 848: '나오고',\n",
       " 849: '있어서',\n",
       " 850: '연기자',\n",
       " 851: '필요',\n",
       " 852: '안된다',\n",
       " 853: '대단한',\n",
       " 854: '역대',\n",
       " 855: '새',\n",
       " 856: '원래',\n",
       " 857: '만큼',\n",
       " 858: '어이',\n",
       " 859: '영웅',\n",
       " 860: '한편',\n",
       " 861: '천재',\n",
       " 862: '엉망',\n",
       " 863: '평',\n",
       " 864: '어쩔',\n",
       " 865: '짓',\n",
       " 866: '상미',\n",
       " 867: '키',\n",
       " 868: '에겐',\n",
       " 869: '스타',\n",
       " 870: '이라니',\n",
       " 871: '뻔',\n",
       " 872: '당',\n",
       " 873: '아빠',\n",
       " 874: '훌륭한',\n",
       " 875: '해준',\n",
       " 876: '복수',\n",
       " 877: '죽음',\n",
       " 878: '스러운',\n",
       " 879: '딸',\n",
       " 880: '유치',\n",
       " 881: '액션영화',\n",
       " 882: '술',\n",
       " 883: '되지',\n",
       " 884: '주의',\n",
       " 885: '수가',\n",
       " 886: '비해',\n",
       " 887: '하는게',\n",
       " 888: '더욱',\n",
       " 889: '있음',\n",
       " 890: '잠',\n",
       " 891: '모르고',\n",
       " 892: '마무리',\n",
       " 893: '우연히',\n",
       " 894: '있게',\n",
       " 895: '조차',\n",
       " 896: '씩',\n",
       " 897: '그래픽',\n",
       " 898: '입',\n",
       " 899: '재미있는',\n",
       " 900: '스타일',\n",
       " 901: '드는',\n",
       " 902: '예고편',\n",
       " 903: '편집',\n",
       " 904: 'good',\n",
       " 905: '댓글',\n",
       " 906: '타',\n",
       " 907: '노',\n",
       " 908: '알았는데',\n",
       " 909: '공',\n",
       " 910: '강',\n",
       " 911: '행동',\n",
       " 912: '였어요',\n",
       " 913: '관',\n",
       " 914: '하며',\n",
       " 915: '보이',\n",
       " 916: '참고',\n",
       " 917: '하는지',\n",
       " 918: '일단',\n",
       " 919: '역',\n",
       " 920: '바보',\n",
       " 921: '억',\n",
       " 922: '먼저',\n",
       " 923: '젠',\n",
       " 924: '물론',\n",
       " 925: '꿀잼',\n",
       " 926: '통해',\n",
       " 927: '억지로',\n",
       " 928: '했음',\n",
       " 929: '전편',\n",
       " 930: '대작',\n",
       " 931: '재미없고',\n",
       " 932: '무비',\n",
       " 933: '화이팅',\n",
       " 934: '없네요',\n",
       " 935: '학교',\n",
       " 936: '나옴',\n",
       " 937: '재미없어',\n",
       " 938: '배',\n",
       " 939: '야할',\n",
       " 940: '봤네요',\n",
       " 941: '진행',\n",
       " 942: '없을',\n",
       " 943: '재미없는',\n",
       " 944: '-_-',\n",
       " 945: '똥',\n",
       " 946: '회',\n",
       " 947: '전작',\n",
       " 948: '알았다',\n",
       " 949: '나올',\n",
       " 950: '한다는',\n",
       " 951: '했으면',\n",
       " 952: '엄청난',\n",
       " 953: '자꾸',\n",
       " 954: '래',\n",
       " 955: '귀',\n",
       " 956: '완벽한',\n",
       " 957: '속편',\n",
       " 958: '긴',\n",
       " 959: '놔',\n",
       " 960: '미화',\n",
       " 961: '척',\n",
       " 962: '게임',\n",
       " 963: '티',\n",
       " 964: '외',\n",
       " 965: '영환',\n",
       " 966: '땐',\n",
       " 967: '결혼',\n",
       " 968: '가끔',\n",
       " 969: '괜히',\n",
       " 970: '화가',\n",
       " 971: '그게',\n",
       " 972: '보는게',\n",
       " 973: '~!!',\n",
       " 974: '메세지',\n",
       " 975: '중반',\n",
       " 976: '같아',\n",
       " 977: '막',\n",
       " 978: '손',\n",
       " 979: '낚',\n",
       " 980: '노출',\n",
       " 981: '있을까',\n",
       " 982: '이래',\n",
       " 983: '기대하고',\n",
       " 984: '효과',\n",
       " 985: '힘든',\n",
       " 986: '심리',\n",
       " 987: '경찰',\n",
       " 988: '자극',\n",
       " 989: '제작비',\n",
       " 990: '동',\n",
       " 991: '로는',\n",
       " 992: '깊게',\n",
       " 993: '으',\n",
       " 994: '쓴',\n",
       " 995: '어디서',\n",
       " 996: '리메이크',\n",
       " 997: '오빠',\n",
       " 998: '♡',\n",
       " 999: '아저씨',\n",
       " ...}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "75f64e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in id_to_token.items():\n",
    "    if token not in ['<pad>', '<unk>']:\n",
    "        init_embeddings[index] = word2vec.wv[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "189abb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b8e1e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type='lstm',\n",
    "        pretrained_embedding=None  # 새로운 매개변수 추가\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=n_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=0\n",
    "            )\n",
    "        \n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e2bff335",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sentence_classifier(\n",
    "    n_vocab = n_vocab,\n",
    "    hidden_dim = hidden_dim,\n",
    "    embedding_dim = embedding_dim,\n",
    "    n_layers = n_layers,\n",
    "    pretrained_embedding = init_embeddings,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c06f2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b817c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0: 0.7015191316604614\n",
      "train_loss 500: 0.6934962734252869\n",
      "train_loss 1000: 0.6934195609835835\n",
      "train_loss 1500: 0.6934081540752617\n",
      "train_loss 2000: 0.6934300783215493\n",
      "train_loss 2500: 0.6934670106929\n",
      "Test Loss: 0.6936484344851095, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.6969184875488281\n",
      "train_loss 500: 0.6935072561462007\n",
      "train_loss 1000: 0.6934360885596299\n",
      "train_loss 1500: 0.6932757778377393\n",
      "train_loss 2000: 0.6932530745096889\n",
      "train_loss 2500: 0.6932658483294762\n",
      "Test Loss: 0.6936503962967724, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.6907771229743958\n",
      "train_loss 500: 0.6932430875277567\n",
      "train_loss 1000: 0.6932838079812643\n",
      "train_loss 1500: 0.6933793173004674\n",
      "train_loss 2000: 0.6933697291102069\n",
      "train_loss 2500: 0.6933468011845021\n",
      "Test Loss: 0.6936381263093064, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.6913106441497803\n",
      "train_loss 500: 0.6931516592374105\n",
      "train_loss 1000: 0.6932619132481136\n",
      "train_loss 1500: 0.6934792202842148\n",
      "train_loss 2000: 0.6934826141712012\n",
      "train_loss 2500: 0.6934723247055624\n",
      "Test Loss: 0.6936495553571195, Test Accuracy: 0.4821999967098236\n",
      "train_loss 0: 0.688490092754364\n",
      "train_loss 500: 0.6935064494014976\n",
      "train_loss 1000: 0.6933467661465084\n",
      "train_loss 1500: 0.6934282658975336\n",
      "train_loss 2000: 0.6934306622743011\n",
      "train_loss 2500: 0.6934671546163105\n",
      "Test Loss: 0.6936352637629158, Test Accuracy: 0.4821999967098236\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, device, optimizer, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5d9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
