{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3218e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b6144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de315173",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ceb599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4409, 0.0523]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6a5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac48af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "men = stats.norm.rvs(loc = 175, scale=10, size=500)\n",
    "women = stats.norm.rvs(loc = 160, scale=10, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ebf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([men, women])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86a2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['man'] * len(men) + ['women']*len(women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6699198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'man',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women',\n",
       " 'women']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b78c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(x, y)), columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d185e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\dohyeong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_base.py:948: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='x', ylabel='Density'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(data=df, x='x', hue='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a76e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.43694879059355\n",
      "9.574390602517901e-104\n"
     ]
    }
   ],
   "source": [
    "stats, p_v = stats.ttest_ind(men, women, equal_var=True)\n",
    "print(stats)\n",
    "print(p_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddc487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[i] for i in range(1, 31)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de363553",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1], [8.46], [9.5], [10.67], [11.16], [14], [11.83],\n",
    "             [14.4], [14.25], [16.2], [16.32], [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f08548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0\n",
    "bias = 0\n",
    "learning_rate=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2863fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  100, weight : 0.858, bias : -0.011, cost : 1.415\n",
      "Epoch :  200, weight : 0.860, bias : -0.059, cost : 1.406\n",
      "Epoch :  300, weight : 0.862, bias : -0.101, cost : 1.399\n",
      "Epoch :  400, weight : 0.864, bias : -0.139, cost : 1.393\n",
      "Epoch :  500, weight : 0.866, bias : -0.172, cost : 1.389\n",
      "Epoch :  600, weight : 0.867, bias : -0.201, cost : 1.385\n",
      "Epoch :  700, weight : 0.869, bias : -0.228, cost : 1.383\n",
      "Epoch :  800, weight : 0.870, bias : -0.251, cost : 1.380\n",
      "Epoch :  900, weight : 0.871, bias : -0.272, cost : 1.379\n",
      "Epoch : 1000, weight : 0.872, bias : -0.290, cost : 1.377\n",
      "Epoch : 1100, weight : 0.873, bias : -0.306, cost : 1.376\n",
      "Epoch : 1200, weight : 0.873, bias : -0.321, cost : 1.375\n",
      "Epoch : 1300, weight : 0.874, bias : -0.334, cost : 1.375\n",
      "Epoch : 1400, weight : 0.874, bias : -0.345, cost : 1.374\n",
      "Epoch : 1500, weight : 0.875, bias : -0.355, cost : 1.374\n",
      "Epoch : 1600, weight : 0.875, bias : -0.364, cost : 1.373\n",
      "Epoch : 1700, weight : 0.876, bias : -0.372, cost : 1.373\n",
      "Epoch : 1800, weight : 0.876, bias : -0.379, cost : 1.373\n",
      "Epoch : 1900, weight : 0.876, bias : -0.386, cost : 1.373\n",
      "Epoch : 2000, weight : 0.877, bias : -0.391, cost : 1.373\n",
      "Epoch : 2100, weight : 0.877, bias : -0.396, cost : 1.373\n",
      "Epoch : 2200, weight : 0.877, bias : -0.401, cost : 1.372\n",
      "Epoch : 2300, weight : 0.877, bias : -0.405, cost : 1.372\n",
      "Epoch : 2400, weight : 0.878, bias : -0.408, cost : 1.372\n",
      "Epoch : 2500, weight : 0.878, bias : -0.411, cost : 1.372\n",
      "Epoch : 2600, weight : 0.878, bias : -0.414, cost : 1.372\n",
      "Epoch : 2700, weight : 0.878, bias : -0.417, cost : 1.372\n",
      "Epoch : 2800, weight : 0.878, bias : -0.419, cost : 1.372\n",
      "Epoch : 2900, weight : 0.878, bias : -0.421, cost : 1.372\n",
      "Epoch : 3000, weight : 0.878, bias : -0.422, cost : 1.372\n",
      "Epoch : 3100, weight : 0.878, bias : -0.424, cost : 1.372\n",
      "Epoch : 3200, weight : 0.878, bias : -0.425, cost : 1.372\n",
      "Epoch : 3300, weight : 0.878, bias : -0.426, cost : 1.372\n",
      "Epoch : 3400, weight : 0.878, bias : -0.427, cost : 1.372\n",
      "Epoch : 3500, weight : 0.879, bias : -0.428, cost : 1.372\n",
      "Epoch : 3600, weight : 0.879, bias : -0.429, cost : 1.372\n",
      "Epoch : 3700, weight : 0.879, bias : -0.430, cost : 1.372\n",
      "Epoch : 3800, weight : 0.879, bias : -0.431, cost : 1.372\n",
      "Epoch : 3900, weight : 0.879, bias : -0.431, cost : 1.372\n",
      "Epoch : 4000, weight : 0.879, bias : -0.432, cost : 1.372\n",
      "Epoch : 4100, weight : 0.879, bias : -0.432, cost : 1.372\n",
      "Epoch : 4200, weight : 0.879, bias : -0.433, cost : 1.372\n",
      "Epoch : 4300, weight : 0.879, bias : -0.433, cost : 1.372\n",
      "Epoch : 4400, weight : 0.879, bias : -0.433, cost : 1.372\n",
      "Epoch : 4500, weight : 0.879, bias : -0.434, cost : 1.372\n",
      "Epoch : 4600, weight : 0.879, bias : -0.434, cost : 1.372\n",
      "Epoch : 4700, weight : 0.879, bias : -0.434, cost : 1.372\n",
      "Epoch : 4800, weight : 0.879, bias : -0.434, cost : 1.372\n",
      "Epoch : 4900, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5000, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5100, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5200, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5300, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5400, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5500, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5600, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5700, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5800, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 5900, weight : 0.879, bias : -0.435, cost : 1.372\n",
      "Epoch : 6000, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6100, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6200, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6300, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6400, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6500, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6600, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6700, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6800, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 6900, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7000, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7100, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7200, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7300, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7400, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7500, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7600, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7700, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7800, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 7900, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8000, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8100, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8200, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8300, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8400, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8500, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8600, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8700, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8800, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 8900, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9000, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9100, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9200, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9300, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9400, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9500, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9600, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9700, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9800, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 9900, weight : 0.879, bias : -0.436, cost : 1.372\n",
      "Epoch : 10000, weight : 0.879, bias : -0.436, cost : 1.372\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    y_hat = weight*x+bias\n",
    "    cost = ((y-y_hat)**2).mean()\n",
    "    \n",
    "    weight = weight - learning_rate*((y_hat-y)*x).mean()\n",
    "    bias = bias - learning_rate*(y_hat-y).mean()\n",
    "    \n",
    "    if (epoch + 1)%100 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, weight : {weight:.3f}, bias : {bias:.3f}, cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1effbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ee99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(x)\n",
    "y = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6431d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.zeros(1, requires_grad = True)\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b24da3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([weight, bias], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd0c488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  500, weight : 0.860, bias : -0.059, cost : 1.406\n",
      "Epoch : 1000, weight : 0.864, bias : -0.138, cost : 1.393\n",
      "Epoch : 1500, weight : 0.867, bias : -0.201, cost : 1.385\n",
      "Epoch : 2000, weight : 0.870, bias : -0.251, cost : 1.380\n",
      "Epoch : 2500, weight : 0.872, bias : -0.290, cost : 1.377\n",
      "Epoch : 3000, weight : 0.873, bias : -0.321, cost : 1.375\n",
      "Epoch : 3500, weight : 0.874, bias : -0.345, cost : 1.374\n",
      "Epoch : 4000, weight : 0.875, bias : -0.364, cost : 1.373\n",
      "Epoch : 4500, weight : 0.876, bias : -0.379, cost : 1.373\n",
      "Epoch : 5000, weight : 0.877, bias : -0.391, cost : 1.373\n",
      "Epoch : 5500, weight : 0.877, bias : -0.401, cost : 1.372\n",
      "Epoch : 6000, weight : 0.878, bias : -0.408, cost : 1.372\n",
      "Epoch : 6500, weight : 0.878, bias : -0.414, cost : 1.372\n",
      "Epoch : 7000, weight : 0.878, bias : -0.419, cost : 1.372\n",
      "Epoch : 7500, weight : 0.878, bias : -0.422, cost : 1.372\n",
      "Epoch : 8000, weight : 0.878, bias : -0.425, cost : 1.372\n",
      "Epoch : 8500, weight : 0.878, bias : -0.427, cost : 1.372\n",
      "Epoch : 9000, weight : 0.879, bias : -0.429, cost : 1.372\n",
      "Epoch : 9500, weight : 0.879, bias : -0.431, cost : 1.372\n",
      "Epoch : 10000, weight : 0.879, bias : -0.432, cost : 1.372\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    hypothesis = x*weight+bias\n",
    "    cost = torch.mean((hypothesis - y)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1)%500 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, weight : {weight.item():.3f}, bias : {bias.item():.3f}, cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c29e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b3dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1, bias=True)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8a8110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MSELoss in module torch.nn.modules.loss:\n",
      "\n",
      "class MSELoss(_Loss)\n",
      " |  MSELoss(size_average=None, reduce=None, reduction: str = 'mean') -> None\n",
      " |  \n",
      " |  Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
      " |  each element in the input :math:`x` and target :math:`y`.\n",
      " |  \n",
      " |  The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      " |      l_n = \\left( x_n - y_n \\right)^2,\n",
      " |  \n",
      " |  where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n",
      " |  (default ``'mean'``), then:\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\ell(x, y) =\n",
      " |      \\begin{cases}\n",
      " |          \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n",
      " |          \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n",
      " |      \\end{cases}\n",
      " |  \n",
      " |  :math:`x` and :math:`y` are tensors of arbitrary shapes with a total\n",
      " |  of :math:`n` elements each.\n",
      " |  \n",
      " |  The mean operation still operates over all the elements, and divides by :math:`n`.\n",
      " |  \n",
      " |  The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n",
      " |  \n",
      " |  Args:\n",
      " |      size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      " |          the losses are averaged over each loss element in the batch. Note that for\n",
      " |          some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
      " |          is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      " |          when :attr:`reduce` is ``False``. Default: ``True``\n",
      " |      reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      " |          losses are averaged or summed over observations for each minibatch depending\n",
      " |          on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      " |          batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      " |      reduction (str, optional): Specifies the reduction to apply to the output:\n",
      " |          ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      " |          ``'mean'``: the sum of the output will be divided by the number of\n",
      " |          elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      " |          and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      " |          specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      " |      - Target: :math:`(*)`, same shape as the input.\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> loss = nn.MSELoss()\n",
      " |      >>> input = torch.randn(3, 5, requires_grad=True)\n",
      " |      >>> target = torch.randn(3, 5)\n",
      " |      >>> output = loss(input, target)\n",
      " |      >>> output.backward()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MSELoss\n",
      " |      _Loss\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __constants__ = ['reduction']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): whether to assign items in the state\n",
      " |              dictionary to their corresponding keys in the module instead\n",
      " |              of copying them inplace into the module's current parameters and buffers.\n",
      " |              When ``False``, the properties of the tensors in the current\n",
      " |              module are preserved while when ``True``, the properties of the\n",
      " |              Tensors in the state dict are preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Resets gradients of all model parameters. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c6e273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  500, Model : [Parameter containing:\n",
      "tensor([[0.8262]], requires_grad=True), Parameter containing:\n",
      "tensor([0.6344], requires_grad=True)], cost : 1.645\n",
      "Epoch : 1000, Model : [Parameter containing:\n",
      "tensor([[0.8373]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4084], requires_grad=True)], cost : 1.542\n",
      "Epoch : 1500, Model : [Parameter containing:\n",
      "tensor([[0.8461]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2301], requires_grad=True)], cost : 1.478\n",
      "Epoch : 2000, Model : [Parameter containing:\n",
      "tensor([[0.8530]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0894], requires_grad=True)], cost : 1.438\n",
      "Epoch : 2500, Model : [Parameter containing:\n",
      "tensor([[0.8585]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0215], requires_grad=True)], cost : 1.413\n",
      "Epoch : 3000, Model : [Parameter containing:\n",
      "tensor([[0.8628]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1090], requires_grad=True)], cost : 1.398\n",
      "Epoch : 3500, Model : [Parameter containing:\n",
      "tensor([[0.8662]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1780], requires_grad=True)], cost : 1.388\n",
      "Epoch : 4000, Model : [Parameter containing:\n",
      "tensor([[0.8689]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2325], requires_grad=True)], cost : 1.382\n",
      "Epoch : 4500, Model : [Parameter containing:\n",
      "tensor([[0.8710]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2754], requires_grad=True)], cost : 1.378\n",
      "Epoch : 5000, Model : [Parameter containing:\n",
      "tensor([[0.8727]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3093], requires_grad=True)], cost : 1.376\n",
      "Epoch : 5500, Model : [Parameter containing:\n",
      "tensor([[0.8740]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3361], requires_grad=True)], cost : 1.375\n",
      "Epoch : 6000, Model : [Parameter containing:\n",
      "tensor([[0.8750]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3572], requires_grad=True)], cost : 1.374\n",
      "Epoch : 6500, Model : [Parameter containing:\n",
      "tensor([[0.8758]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3738], requires_grad=True)], cost : 1.373\n",
      "Epoch : 7000, Model : [Parameter containing:\n",
      "tensor([[0.8765]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3869], requires_grad=True)], cost : 1.373\n",
      "Epoch : 7500, Model : [Parameter containing:\n",
      "tensor([[0.8770]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3973], requires_grad=True)], cost : 1.373\n",
      "Epoch : 8000, Model : [Parameter containing:\n",
      "tensor([[0.8774]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4054], requires_grad=True)], cost : 1.372\n",
      "Epoch : 8500, Model : [Parameter containing:\n",
      "tensor([[0.8777]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4119], requires_grad=True)], cost : 1.372\n",
      "Epoch : 9000, Model : [Parameter containing:\n",
      "tensor([[0.8780]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4169], requires_grad=True)], cost : 1.372\n",
      "Epoch : 9500, Model : [Parameter containing:\n",
      "tensor([[0.8782]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4210], requires_grad=True)], cost : 1.372\n",
      "Epoch : 10000, Model : [Parameter containing:\n",
      "tensor([[0.8783]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4241], requires_grad=True)], cost : 1.372\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    output = model(x)\n",
    "    cost = criterion(output, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1)%500 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Model : {list(model.parameters())}, cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54f908ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c570247",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.FloatTensor([\n",
    "    [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]\n",
    "])\n",
    "\n",
    "train_y = torch.FloatTensor([\n",
    "    [0.1, 1.5], [1, 2.8], [1.9, 4.1], [2.8, 5.4], [3.7, 6.7], [4.6, 8]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c772f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbae6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(2, 2, bias=True)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f201608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  500, Model : [Parameter containing:\n",
      "tensor([[0.4589, 0.2405],\n",
      "        [0.7342, 0.5631]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1959, -0.3519], requires_grad=True)], cost : 0.070\n",
      "Epoch : 1000, Model : [Parameter containing:\n",
      "tensor([[0.5777, 0.1791],\n",
      "        [0.7358, 0.5623]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3762, -0.3543], requires_grad=True)], cost : 0.036\n",
      "Epoch : 1500, Model : [Parameter containing:\n",
      "tensor([[0.6627, 0.1353],\n",
      "        [0.7369, 0.5617]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5049, -0.3560], requires_grad=True)], cost : 0.018\n",
      "Epoch : 2000, Model : [Parameter containing:\n",
      "tensor([[0.7231, 0.1039],\n",
      "        [0.7377, 0.5613]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5968, -0.3572], requires_grad=True)], cost : 0.009\n",
      "Epoch : 2500, Model : [Parameter containing:\n",
      "tensor([[0.7664, 0.0816],\n",
      "        [0.7383, 0.5610]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.6623, -0.3581], requires_grad=True)], cost : 0.005\n",
      "Epoch : 3000, Model : [Parameter containing:\n",
      "tensor([[0.7972, 0.0657],\n",
      "        [0.7387, 0.5608]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7091, -0.3587], requires_grad=True)], cost : 0.002\n",
      "Epoch : 3500, Model : [Parameter containing:\n",
      "tensor([[0.8192, 0.0542],\n",
      "        [0.7390, 0.5606]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7425, -0.3592], requires_grad=True)], cost : 0.001\n",
      "Epoch : 4000, Model : [Parameter containing:\n",
      "tensor([[0.8350, 0.0461],\n",
      "        [0.7392, 0.5605]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7664, -0.3595], requires_grad=True)], cost : 0.001\n",
      "Epoch : 4500, Model : [Parameter containing:\n",
      "tensor([[0.8462, 0.0403],\n",
      "        [0.7394, 0.5605]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7834, -0.3597], requires_grad=True)], cost : 0.000\n",
      "Epoch : 5000, Model : [Parameter containing:\n",
      "tensor([[0.8542, 0.0362],\n",
      "        [0.7395, 0.5604]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7956, -0.3599], requires_grad=True)], cost : 0.000\n",
      "Epoch : 5500, Model : [Parameter containing:\n",
      "tensor([[0.8599, 0.0332],\n",
      "        [0.7396, 0.5604]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8042, -0.3600], requires_grad=True)], cost : 0.000\n",
      "Epoch : 6000, Model : [Parameter containing:\n",
      "tensor([[0.8640, 0.0311],\n",
      "        [0.7396, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8104, -0.3601], requires_grad=True)], cost : 0.000\n",
      "Epoch : 6500, Model : [Parameter containing:\n",
      "tensor([[0.8669, 0.0296],\n",
      "        [0.7396, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8148, -0.3601], requires_grad=True)], cost : 0.000\n",
      "Epoch : 7000, Model : [Parameter containing:\n",
      "tensor([[0.8690, 0.0285],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8180, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 7500, Model : [Parameter containing:\n",
      "tensor([[0.8704, 0.0278],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8202, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 8000, Model : [Parameter containing:\n",
      "tensor([[0.8715, 0.0272],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8218, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 8500, Model : [Parameter containing:\n",
      "tensor([[0.8723, 0.0268],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8230, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 9000, Model : [Parameter containing:\n",
      "tensor([[0.8728, 0.0265],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8238, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 9500, Model : [Parameter containing:\n",
      "tensor([[0.8732, 0.0263],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8244, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 10000, Model : [Parameter containing:\n",
      "tensor([[0.8735, 0.0262],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8248, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 10500, Model : [Parameter containing:\n",
      "tensor([[0.8737, 0.0261],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8251, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 11000, Model : [Parameter containing:\n",
      "tensor([[0.8738, 0.0260],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8253, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 11500, Model : [Parameter containing:\n",
      "tensor([[0.8739, 0.0260],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8255, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 12000, Model : [Parameter containing:\n",
      "tensor([[0.8740, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8256, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 12500, Model : [Parameter containing:\n",
      "tensor([[0.8740, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8257, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 13000, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8257, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 13500, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8257, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 14000, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 14500, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 15000, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 15500, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 16000, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0259],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 16500, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 17000, Model : [Parameter containing:\n",
      "tensor([[0.8741, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 17500, Model : [Parameter containing:\n",
      "tensor([[0.8742, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 18000, Model : [Parameter containing:\n",
      "tensor([[0.8742, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 18500, Model : [Parameter containing:\n",
      "tensor([[0.8742, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 19000, Model : [Parameter containing:\n",
      "tensor([[0.8742, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 19500, Model : [Parameter containing:\n",
      "tensor([[0.8742, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n",
      "Epoch : 20000, Model : [Parameter containing:\n",
      "tensor([[0.8742, 0.0258],\n",
      "        [0.7397, 0.5603]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8258, -0.3602], requires_grad=True)], cost : 0.000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20000):\n",
    "    cost = 0.0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        x, y = batch\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cost += loss\n",
    "        \n",
    "    cost = cost/len(train_dataloader)\n",
    "    \n",
    "    if (epoch + 1)%500 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Model : {list(model.parameters())}, cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a47150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16a07bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c08ba796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6ccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a36ea2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.x = df.iloc[:, 0].values\n",
    "        self.y = df.iloc[:, 0].values\n",
    "        self.length = len(df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor([self.x[index]  ** 2, self.x[index]])\n",
    "        y = torch.FloatTensor([self.y[index]])\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc5d7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a979d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5feb253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('./non_linear.csv')\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "validation_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last = True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "882da01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs : tensor([[57.7413, 44.7849],\n",
      "        [ 0.6851,  1.6182],\n",
      "        [16.7598, 12.0980],\n",
      "        [ 2.7225,  3.7192]])\n",
      "outputs : tensor([[-0.8145, -0.2968],\n",
      "        [ 4.7023,  5.6637],\n",
      "        [ 5.0164,  3.1836],\n",
      "        [ 6.6008,  7.4856]])\n",
      "outputs : tensor([[-0.0945,  0.7432],\n",
      "        [27.7231, 26.8671],\n",
      "        [53.5268, 41.3801],\n",
      "        [62.1131, 48.3228]])\n",
      "outputs : tensor([[23.5926, 17.4493],\n",
      "        [30.7992, 29.6316],\n",
      "        [81.6750, 74.6754],\n",
      "        [-0.7394, -0.1256]])\n",
      "outputs : tensor([[ 3.4620,  4.4526],\n",
      "        [49.8623, 46.6245],\n",
      "        [ 3.3483,  1.9874],\n",
      "        [38.2424, 29.0924]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for x, y in validation_dataloader :\n",
    "        \n",
    "        outputs = model(x)\n",
    "        print(f'outputs : {outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d844c",
   "metadata": {},
   "source": [
    "  (random_split)\n",
    "```python\n",
    "subset = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    lengths(list),\n",
    "    generator\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220a64e",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c337a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        self.x1 = df.iloc[:, 0].values\n",
    "        self.x2 = df.iloc[:, 1].values\n",
    "        self.x3 = df.iloc[:, 2].values\n",
    "        self.y = df.iloc[:, 3].values\n",
    "        self.length = len(df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor([self.x1[index], self.x2[index], self.x3[index]])\n",
    "        y = torch.FloatTensor([self.y[index]])\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, y):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cf3633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01373bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
